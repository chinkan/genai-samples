{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen Simplified Multi-Agent Research System\n",
    "\n",
    "This is a **simplified and clean** version that removes all fallback mechanisms and focuses on a properly configured SelectorGroupChat for reliable multi-agent coordination.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "1. **Simplified Configuration**: Single model (Claude 3.5 Sonnet) for all agents to eliminate compatibility issues\n",
    "2. **Clear Agent Handoffs**: Explicit handoff signals between agents \n",
    "3. **Optimized Selector Prompt**: Better agent selection logic\n",
    "4. **No Fallback Complexity**: Clean, focused implementation\n",
    "\n",
    "## Architecture\n",
    "- **Planning Agent**: Creates research plans and identifies sources\n",
    "- **Web Search Agent**: Scrapes websites for content  \n",
    "- **Citation Agent**: Validates sources and creates citations\n",
    "- **Finalize Agent**: Creates comprehensive final reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogen-agentchat in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: autogen-ext[openai] in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: autogen-core==0.7.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-agentchat) (0.7.1)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (0.4.1)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (24.1.0)\n",
      "Requirement already satisfied: openai>=1.93 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (1.98.0)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (0.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.1->autogen-agentchat) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.1->autogen-agentchat) (3.23.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2025.7.34)\n",
      "Requirement already satisfied: colorama in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.93->autogen-ext[openai]) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" requests beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AutoGen v0.4 imports\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import ModelInfo, ModelFamily\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration validated successfully!\n",
      "Using model: anthropic/claude-sonnet-4 for all agents\n",
      "OpenRouter Base URL: https://openrouter.ai/api/v1\n"
     ]
    }
   ],
   "source": [
    "# Simplified Configuration - Using Claude 3.5 Sonnet for all agents for consistency\n",
    "SCRAPINGDOG_API_KEY = os.getenv(\"SCRAPINGDOG_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "\n",
    "# Use single reliable model for all agents\n",
    "MODEL_NAME = \"anthropic/claude-sonnet-4\"\n",
    "\n",
    "# Validate required environment variables\n",
    "required_vars = {\n",
    "    \"SCRAPINGDOG_API_KEY\": SCRAPINGDOG_API_KEY,\n",
    "    \"OPENROUTER_API_KEY\": OPENROUTER_API_KEY\n",
    "}\n",
    "\n",
    "missing_vars = [var for var, value in required_vars.items() if not value]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"Configuration validated successfully!\")\n",
    "print(f\"Using model: {MODEL_NAME} for all agents\")\n",
    "print(f\"OpenRouter Base URL: {OPENROUTER_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScrapingDog client initialized!\n"
     ]
    }
   ],
   "source": [
    "# ScrapingDog API Client with Google Search functionality\n",
    "class ScrapingDogClient:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.scrape_url_endpoint = \"https://api.scrapingdog.com/scrape\"\n",
    "        self.google_search_endpoint = \"https://api.scrapingdog.com/google_search\"\n",
    "        self.google_scholar_endpoint = \"https://api.scrapingdog.com/google_scholar\"\n",
    "        self.google_news_endpoint = \"https://api.scrapingdog.com/google_news\"\n",
    "    \n",
    "    def search_google(self, query: str, num_results: int = 10, country: str = \"us\") -> Dict[str, Any]:\n",
    "        \"\"\"Search Google using ScrapingDog Google Search API\"\"\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"q\": query,\n",
    "            \"num\": num_results,\n",
    "            \"gl\": country\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.google_search_endpoint, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                search_data = response.json()\n",
    "                urls = []\n",
    "                \n",
    "                # Extract URLs from organic results\n",
    "                if 'organic_results' in search_data:\n",
    "                    for result in search_data['organic_results'][:num_results]:\n",
    "                        urls.append({\n",
    "                            \"url\": result.get('link', ''),\n",
    "                            \"title\": result.get('title', ''),\n",
    "                            \"snippet\": result.get('snippet', ''),\n",
    "                            \"rank\": result.get('rank', 0)\n",
    "                        })\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"query\": query,\n",
    "                    \"urls\": urls,\n",
    "                    \"total_results\": len(urls),\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"query\": query,\n",
    "                    \"error\": f\"Search failed: HTTP {response.status_code}\",\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"query\": query,\n",
    "                \"error\": f\"Search error: {str(e)}\",\n",
    "                \"searched_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def search_google_scholar(self, query: str, num_results: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Search Google Scholar using ScrapingDog API\"\"\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"q\": query,\n",
    "            \"num\": num_results\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.google_scholar_endpoint, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                scholar_data = response.json()\n",
    "                urls = []\n",
    "                \n",
    "                # Extract URLs from scholar results\n",
    "                if 'organic_results' in scholar_data:\n",
    "                    for result in scholar_data['organic_results'][:num_results]:\n",
    "                        if result.get('link'):\n",
    "                            urls.append({\n",
    "                                \"url\": result.get('link', ''),\n",
    "                                \"title\": result.get('title', ''),\n",
    "                                \"snippet\": result.get('snippet', ''),\n",
    "                                \"authors\": result.get('authors', ''),\n",
    "                                \"year\": result.get('year', ''),\n",
    "                                \"citations\": result.get('citations', ''),\n",
    "                                \"type\": \"academic\"\n",
    "                            })\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"query\": query,\n",
    "                    \"urls\": urls,\n",
    "                    \"total_results\": len(urls),\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"query\": query,\n",
    "                    \"error\": f\"Scholar search failed: HTTP {response.status_code}\",\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"query\": query,\n",
    "                \"error\": f\"Scholar search error: {str(e)}\",\n",
    "                \"searched_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def search_google_news(self, query: str, num_results: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Search Google News using ScrapingDog API\"\"\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"q\": query,\n",
    "            \"num\": num_results\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.google_news_endpoint, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                news_data = response.json()\n",
    "                urls = []\n",
    "                \n",
    "                # Extract URLs from news results\n",
    "                if 'news_results' in news_data:\n",
    "                    for result in news_data['news_results'][:num_results]:\n",
    "                        urls.append({\n",
    "                            \"url\": result.get('link', ''),\n",
    "                            \"title\": result.get('title', ''),\n",
    "                            \"snippet\": result.get('snippet', ''),\n",
    "                            \"source\": result.get('source', ''),\n",
    "                            \"date\": result.get('date', ''),\n",
    "                            \"type\": \"news\"\n",
    "                        })\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"query\": query,\n",
    "                    \"urls\": urls,\n",
    "                    \"total_results\": len(urls),\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"query\": query,\n",
    "                    \"error\": f\"News search failed: HTTP {response.status_code}\",\n",
    "                    \"searched_at\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"query\": query,\n",
    "                \"error\": f\"News search error: {str(e)}\",\n",
    "                \"searched_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def scrape_url(self, url: str, render_js: bool = True, country: str = \"US\") -> Dict[str, Any]:\n",
    "        \"\"\"Scrape a URL using ScrapingDog API\"\"\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"url\": url,\n",
    "            \"dynamic\": \"true\" if render_js else \"false\",\n",
    "            \"country\": country\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.scrape_url_endpoint, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                text_content = soup.get_text(separator=' ', strip=True)\n",
    "                title = soup.find('title')\n",
    "                title_text = title.get_text().strip() if title else \"No title found\"\n",
    "                meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "                description = meta_desc.get('content', '') if meta_desc else ''\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"url\": url,\n",
    "                    \"title\": title_text,\n",
    "                    \"description\": description,\n",
    "                    \"content\": text_content[:5000],\n",
    "                    \"scraped_at\": datetime.now().isoformat()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"url\": url,\n",
    "                    \"error\": f\"HTTP {response.status_code}: {response.text}\",\n",
    "                    \"scraped_at\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"url\": url,\n",
    "                \"error\": str(e),\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize ScrapingDog client\n",
    "scraping_client = ScrapingDogClient(SCRAPINGDOG_API_KEY)\n",
    "print(\"ScrapingDog client initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating OpenRouter model client...\n",
      "Model client created successfully: anthropic/claude-sonnet-4\n"
     ]
    }
   ],
   "source": [
    "# Create OpenRouter client\n",
    "def create_openrouter_client(model_name: str) -> OpenAIChatCompletionClient:\n",
    "    \"\"\"Create an OpenRouter client for a specific model\"\"\"\n",
    "    model_info = ModelInfo(\n",
    "        vision=False,\n",
    "        function_calling=True,\n",
    "        json_output=True,\n",
    "        family=ModelFamily.CLAUDE_4_SONNET,\n",
    "        structured_output=True\n",
    "    )\n",
    "    \n",
    "    return OpenAIChatCompletionClient(\n",
    "        model=model_name,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        base_url=OPENROUTER_BASE_URL,\n",
    "        model_info=model_info\n",
    "    )\n",
    "\n",
    "# Create model client (same for all agents)\n",
    "print(\"Creating OpenRouter model client...\")\n",
    "model_client = create_openrouter_client(MODEL_NAME)\n",
    "print(f\"Model client created successfully: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search and scraping tools defined successfully!\n",
      "- search_web_sources(): Performs real Google searches using ScrapingDog APIs\n",
      "- scrape_website(): Extracts content from URLs\n",
      "- create_research_plan(): Creates structured research plans\n"
     ]
    }
   ],
   "source": [
    "# Web scraping and search tools\n",
    "def search_web_sources(query: str, include_academic: bool = True, include_news: bool = True) -> str:\n",
    "    \"\"\"Search for relevant web sources using ScrapingDog Google Search APIs.\"\"\"\n",
    "    results = []\n",
    "    all_urls = []\n",
    "    \n",
    "    try:\n",
    "        # Search general web sources\n",
    "        web_search = scraping_client.search_google(query, num_results=5)\n",
    "        if web_search[\"success\"]:\n",
    "            results.append(f\"WEB SEARCH RESULTS ({len(web_search['urls'])} found):\")\n",
    "            for i, url_info in enumerate(web_search['urls'], 1):\n",
    "                results.append(f\"{i}. {url_info['title']}\")\n",
    "                results.append(f\"   URL: {url_info['url']}\")\n",
    "                results.append(f\"   Snippet: {url_info['snippet'][:200]}...\")\n",
    "                all_urls.append(url_info['url'])\n",
    "            results.append(\"\")\n",
    "        else:\n",
    "            results.append(f\"WEB SEARCH FAILED: {web_search['error']}\")\n",
    "        \n",
    "        # Search academic sources if requested\n",
    "        if include_academic:\n",
    "            scholar_search = scraping_client.search_google_scholar(query, num_results=3)\n",
    "            if scholar_search[\"success\"]:\n",
    "                results.append(f\"ACADEMIC SEARCH RESULTS ({len(scholar_search['urls'])} found):\")\n",
    "                for i, url_info in enumerate(scholar_search['urls'], 1):\n",
    "                    results.append(f\"{i}. {url_info['title']}\")\n",
    "                    if url_info.get('authors'):\n",
    "                        results.append(f\"   Authors: {url_info['authors']}\")\n",
    "                    if url_info.get('year'):\n",
    "                        results.append(f\"   Year: {url_info['year']}\")\n",
    "                    results.append(f\"   URL: {url_info['url']}\")\n",
    "                    all_urls.append(url_info['url'])\n",
    "                results.append(\"\")\n",
    "            else:\n",
    "                results.append(f\"ACADEMIC SEARCH FAILED: {scholar_search['error']}\")\n",
    "        \n",
    "        # Search news sources if requested\n",
    "        if include_news:\n",
    "            news_search = scraping_client.search_google_news(query, num_results=3)\n",
    "            if news_search[\"success\"]:\n",
    "                results.append(f\"NEWS SEARCH RESULTS ({len(news_search['urls'])} found):\")\n",
    "                for i, url_info in enumerate(news_search['urls'], 1):\n",
    "                    results.append(f\"{i}. {url_info['title']}\")\n",
    "                    if url_info.get('source'):\n",
    "                        results.append(f\"   Source: {url_info['source']}\")\n",
    "                    if url_info.get('date'):\n",
    "                        results.append(f\"   Date: {url_info['date']}\")\n",
    "                    results.append(f\"   URL: {url_info['url']}\")\n",
    "                    all_urls.append(url_info['url'])\n",
    "                results.append(\"\")\n",
    "            else:\n",
    "                results.append(f\"NEWS SEARCH FAILED: {news_search['error']}\")\n",
    "        \n",
    "        # Summary\n",
    "        results.append(f\"SEARCH SUMMARY:\")\n",
    "        results.append(f\"- Total URLs found: {len(all_urls)}\")\n",
    "        results.append(f\"- Search completed at: {datetime.now().isoformat()}\")\n",
    "        results.append(f\"- Next step: Use scrape_website() to extract content from relevant URLs\")\n",
    "        \n",
    "        return \"\\n\".join(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"\"\"SEARCH ERROR for query: {query}\n",
    "ERROR: {str(e)}\n",
    "TIMESTAMP: {datetime.now().isoformat()}\n",
    "STATUS: Search failed, please try again with a different query.\"\"\"\n",
    "\n",
    "def scrape_website(url: str, render_js: bool = True, country: str = \"US\") -> str:\n",
    "    \"\"\"Scrape a website using the ScrapingDog API.\"\"\"\n",
    "    result = scraping_client.scrape_url(url, render_js, country)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        return f\"\"\"SCRAPED CONTENT FROM: {url}\n",
    "\n",
    "TITLE: {result['title']}\n",
    "DESCRIPTION: {result['description']}\n",
    "CONTENT: {result['content']}\n",
    "SCRAPED AT: {result['scraped_at']}\n",
    "\n",
    "STATUS: Content successfully extracted.\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"SCRAPING FAILED FOR: {url}\n",
    "ERROR: {result['error']}\n",
    "SCRAPED AT: {result['scraped_at']}\n",
    "STATUS: Please try a different URL.\"\"\"\n",
    "\n",
    "def create_research_plan(query: str) -> str:\n",
    "    \"\"\"Create a structured research plan for a given query.\"\"\"\n",
    "    return f\"\"\"RESEARCH PLAN FOR: {query}\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\n",
    "PHASE 1: Source Discovery & Identification\n",
    "- Use search_web_sources() to find relevant URLs across:\n",
    "  * General web sources (Google Search)\n",
    "  * Academic sources (Google Scholar)\n",
    "  * News sources (Google News)\n",
    "- Focus on recent, authoritative sources\n",
    "\n",
    "PHASE 2: Content Extraction & Analysis\n",
    "- Use scrape_website() to extract content from discovered URLs\n",
    "- Analyze content for key facts, insights, and data points\n",
    "- Extract relevant quotes and statistics\n",
    "\n",
    "PHASE 3: Source Validation & Citation Management\n",
    "- Verify source credibility and authority\n",
    "- Check publication dates and author credentials\n",
    "- Create properly formatted APA citations\n",
    "- Identify any bias or limitations\n",
    "\n",
    "PHASE 4: Synthesis & Final Report\n",
    "- Synthesize findings from all sources\n",
    "- Create comprehensive analysis with evidence-based conclusions\n",
    "- Structure report with executive summary, methodology, findings, and recommendations\n",
    "\n",
    "METHODOLOGY: Web-first research using ScrapingDog APIs for real-time content discovery and extraction\n",
    "READY FOR IMPLEMENTATION\"\"\"\n",
    "\n",
    "print(\"Web search and scraping tools defined successfully!\")\n",
    "print(\"- search_web_sources(): Performs real Google searches using ScrapingDog APIs\")\n",
    "print(\"- scrape_website(): Extracts content from URLs\")\n",
    "print(\"- create_research_plan(): Creates structured research plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All agents created with real web search functionality!\n",
      "- PlanningAgent: Uses search_web_sources() for real-time web discovery\n",
      "- WebSearchAgent: Uses scrape_website() for content extraction\n",
      "- CitationAgent: Validates sources and creates citations\n",
      "- FinalizeAgent: Creates comprehensive final reports\n"
     ]
    }
   ],
   "source": [
    "# Create agents with real web search functionality\n",
    "planning_agent = AssistantAgent(\n",
    "    name=\"PlanningAgent\",\n",
    "    model_client=model_client,\n",
    "    tools=[create_research_plan, search_web_sources],\n",
    "    description=\"Creates structured research plans and discovers relevant web sources using real-time search.\",\n",
    "    system_message=\"\"\"You are a Planning Agent specialized in research planning and web source discovery.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Create comprehensive research plans using create_research_plan()\n",
    "2. Discover relevant sources using search_web_sources() - this performs REAL Google searches\n",
    "3. Establish research methodology and objectives\n",
    "4. Provide URLs for content extraction\n",
    "\n",
    "Process:\n",
    "- Analyze the research query to understand objectives\n",
    "- Create structured research plan with phases\n",
    "- Use search_web_sources() to find actual, relevant URLs from:\n",
    "  * Google Search (general web sources)\n",
    "  * Google Scholar (academic sources)  \n",
    "  * Google News (current news sources)\n",
    "- End with \"Research plan complete. Now we need to gather content from these sources.\"\n",
    "\n",
    "Always start with \"RESEARCH PLAN:\" and use search_web_sources() to find real URLs, then conclude with clear handoff to content gathering phase.\"\"\"\n",
    ")\n",
    "\n",
    "web_search_agent = AssistantAgent(\n",
    "    name=\"WebSearchAgent\",\n",
    "    model_client=model_client,\n",
    "    tools=[scrape_website],\n",
    "    description=\"Scrapes websites and extracts content from URLs using ScrapingDog API.\",\n",
    "    system_message=\"\"\"You are a Web Search Agent specialized in content extraction and information gathering.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Use scrape_website() to extract content from identified URLs\n",
    "2. Analyze and synthesize key findings from multiple sources\n",
    "3. Extract relevant data, facts, and insights for the research topic\n",
    "4. Organize information for citation and validation\n",
    "\n",
    "Process:\n",
    "- Extract content from each identified source systematically\n",
    "- Summarize key findings and relevant information\n",
    "- Note important quotes, statistics, and claims\n",
    "- End with \"Content extracted, ready for citation review and source validation.\"\n",
    "\n",
    "Always start with \"SEARCH RESULTS:\" and conclude with clear handoff to citation validation phase.\"\"\"\n",
    ")\n",
    "\n",
    "citation_agent = AssistantAgent(\n",
    "    name=\"CitationAgent\",\n",
    "    model_client=model_client,\n",
    "    description=\"Validates sources and creates proper APA citations for research.\",\n",
    "    system_message=\"\"\"You are a Citation Agent specialized in source validation and academic integrity.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Review and validate all sources used in research\n",
    "2. Assess source credibility, authority, and reliability\n",
    "3. Create properly formatted APA citations\n",
    "4. Identify potential bias or limitations in sources\n",
    "\n",
    "Process:\n",
    "- Evaluate each source for credibility and relevance\n",
    "- Check publication dates, author credentials, and publisher reputation\n",
    "- Format citations according to APA standards\n",
    "- Note any source limitations or potential bias\n",
    "- End with \"Sources verified and citations formatted. Ready for final synthesis.\"\n",
    "\n",
    "Always start with \"CITATION REVIEW:\" and conclude with clear handoff to final report synthesis.\"\"\"\n",
    ")\n",
    "\n",
    "finalize_agent = AssistantAgent(\n",
    "    name=\"FinalizeAgent\",\n",
    "    model_client=model_client,\n",
    "    description=\"Synthesizes research findings into comprehensive final reports.\",\n",
    "    system_message=\"\"\"You are a Finalization Agent specialized in research synthesis and report creation.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Synthesize all research findings from previous agents\n",
    "2. Create comprehensive, well-structured final reports\n",
    "3. Provide analytical insights and evidence-based conclusions\n",
    "4. Include executive summary, methodology, findings, and recommendations\n",
    "\n",
    "Process:\n",
    "- Review all research findings and validated sources\n",
    "- Synthesize information into coherent narrative\n",
    "- Provide analytical insights and evidence-based conclusions\n",
    "- Structure report with executive summary, methodology, key findings, analysis, and recommendations\n",
    "- Ensure all sources are properly cited\n",
    "\n",
    "Always start with \"FINAL REPORT:\" and create a complete, professional research document.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"All agents created with real web search functionality!\")\n",
    "print(\"- PlanningAgent: Uses search_web_sources() for real-time web discovery\")\n",
    "print(\"- WebSearchAgent: Uses scrape_website() for content extraction\")\n",
    "print(\"- CitationAgent: Validates sources and creates citations\")\n",
    "print(\"- FinalizeAgent: Creates comprehensive final reports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectorGroupChat created with OPTIMIZED selector prompt!\n",
      "Key improvements:\n",
      "‚úÖ Uses {history} to analyze conversation context\n",
      "‚úÖ Uses {roles} to understand agent capabilities\n",
      "‚úÖ Uses {participants} for available agent selection\n",
      "‚úÖ Provides clear workflow guidance\n",
      "‚úÖ Maintains flexibility with allow_repeated_speaker=True\n"
     ]
    }
   ],
   "source": [
    "# Create SelectorGroupChat with OPTIMIZED selector prompt using history, roles, participants\n",
    "agent_team = SelectorGroupChat(\n",
    "    participants=[\n",
    "        planning_agent,\n",
    "        web_search_agent,\n",
    "        citation_agent,\n",
    "        finalize_agent\n",
    "    ],\n",
    "    model_client=model_client,\n",
    "    termination_condition=MaxMessageTermination(max_messages=15),\n",
    "    allow_repeated_speaker=True,  # Essential for flexibility\n",
    "    selector_prompt=\"\"\"Select the next agent based on the current conversation context and workflow progression.\n",
    "\n",
    "Available agents and their capabilities:\n",
    "{roles}\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "Based on the conversation above, select the most appropriate agent from {participants} to handle the next task.\n",
    "\n",
    "Selection Guidelines:\n",
    "- If research planning is needed or sources need to be identified ‚Üí PlanningAgent\n",
    "- If websites need to be scraped or content needs to be extracted ‚Üí WebSearchAgent  \n",
    "- If sources need validation or citations need to be created ‚Üí CitationAgent\n",
    "- If all information is gathered and a final report needs to be synthesized ‚Üí FinalizeAgent\n",
    "\n",
    "Analyze the conversation flow and choose the agent whose expertise best matches what needs to happen next.\n",
    "\n",
    "Selected agent:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"SelectorGroupChat created with OPTIMIZED selector prompt!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"‚úÖ Uses {history} to analyze conversation context\")\n",
    "print(\"‚úÖ Uses {roles} to understand agent capabilities\") \n",
    "print(\"‚úÖ Uses {participants} for available agent selection\")\n",
    "print(\"‚úÖ Provides clear workflow guidance\")\n",
    "print(\"‚úÖ Maintains flexibility with allow_repeated_speaker=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research function ready with SelectorGroupChat + RoundRobin fallback!\n"
     ]
    }
   ],
   "source": [
    "# Research function with fallback strategy (best practice)\n",
    "async def run_research(research_query: str):\n",
    "    \"\"\"Run research with SelectorGroupChat and RoundRobin fallback\"\"\"\n",
    "    print(f\"\\nStarting Research: {research_query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    task_message = f\"\"\"Please conduct comprehensive research on: {research_query}\n",
    "\n",
    "This is a collaborative multi-agent research session:\n",
    "- Planning Agent: Create research plan and identify sources\n",
    "- Web Search Agent: Extract content from websites\n",
    "- Citation Agent: Validate sources and create citations\n",
    "- Finalize Agent: Create comprehensive final report\n",
    "\n",
    "Work together to produce high-quality research with proper citations.\"\"\"\n",
    "    \n",
    "    message_count = 0\n",
    "    agents_participated = set()\n",
    "    has_final_report = False\n",
    "    \n",
    "    try:\n",
    "        print(\"üß† Using SelectorGroupChat (intelligent agent selection)...\")\n",
    "        async for message in agent_team.run_stream(task=task_message):\n",
    "            message_count += 1\n",
    "            \n",
    "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                agent_name = message.source\n",
    "                agents_participated.add(agent_name)\n",
    "                \n",
    "                if \"FINAL REPORT:\" in message.content:\n",
    "                    has_final_report = True\n",
    "                \n",
    "                print(f\"\\n[{agent_name}] (Message {message_count}):\")\n",
    "                print(\"-\" * 50)\n",
    "                print(message.content)\n",
    "                print()\n",
    "                \n",
    "            elif str(type(message).__name__) == 'TaskResult':\n",
    "                print(f\"\\n[TASK RESULT]: Session completed after {message_count} messages\")\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"SelectorGroupChat error: {e}\")\n",
    "        print(\"üîÑ Falling back to RoundRobin approach...\")\n",
    "        \n",
    "        # Create fallback RoundRobin team\n",
    "        from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "        fallback_team = RoundRobinGroupChat(\n",
    "            participants=[planning_agent, web_search_agent, citation_agent, finalize_agent],\n",
    "            termination_condition=MaxMessageTermination(max_messages=12)\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            message_count = 0\n",
    "            agents_participated = set()\n",
    "            has_final_report = False\n",
    "            \n",
    "            async for message in fallback_team.run_stream(task=task_message):\n",
    "                message_count += 1\n",
    "                \n",
    "                if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                    agent_name = message.source\n",
    "                    agents_participated.add(agent_name)\n",
    "                    \n",
    "                    if \"FINAL REPORT:\" in message.content:\n",
    "                        has_final_report = True\n",
    "                    \n",
    "                    print(f\"\\n[{agent_name}] (Message {message_count}):\")\n",
    "                    print(\"-\" * 50)\n",
    "                    print(message.content)\n",
    "                    print()\n",
    "                    \n",
    "                elif str(type(message).__name__) == 'TaskResult':\n",
    "                    print(f\"\\n[FALLBACK RESULT]: RoundRobin completed after {message_count} messages\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback error: {e2}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Research Session Complete!\")\n",
    "    print(f\"Total messages: {message_count}\")\n",
    "    print(f\"Agents participated: {', '.join(sorted(agents_participated))}\")\n",
    "    print(f\"Final report generated: {'Yes' if has_final_report else 'No'}\")\n",
    "    \n",
    "    success = len(agents_participated) >= 3 and has_final_report\n",
    "    if success:\n",
    "        print(\"‚úÖ SUCCESS: Research completed successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è PARTIAL: Research had some issues\")\n",
    "        print(\"Recommendations:\")\n",
    "        if len(agents_participated) < 3:\n",
    "            print(\"- Increase max_messages to allow more agent participation\")\n",
    "        if not has_final_report:\n",
    "            print(\"- Ensure FinalizeAgent creates final report\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "print(\"Research function ready with SelectorGroupChat + RoundRobin fallback!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing AutoGen Research System with PROVEN Best Practices\n",
      "\n",
      "Starting Research: Latest genai RAG techniques and their impact on IT development industry in 2025\n",
      "================================================================================\n",
      "üß† Using SelectorGroupChat (intelligent agent selection)...\n",
      "\n",
      "[user] (Message 1):\n",
      "--------------------------------------------------\n",
      "Please conduct comprehensive research on: Latest genai RAG techniques and their impact on IT development industry in 2025\n",
      "\n",
      "This is a collaborative multi-agent research session:\n",
      "- Planning Agent: Create research plan and identify sources\n",
      "- Web Search Agent: Extract content from websites\n",
      "- Citation Agent: Validate sources and create citations\n",
      "- Finalize Agent: Create comprehensive final report\n",
      "\n",
      "Work together to produce high-quality research with proper citations.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 2):\n",
      "--------------------------------------------------\n",
      "RESEARCH PLAN:\n",
      "\n",
      "I'll create a comprehensive research plan for the latest GenAI RAG techniques and their impact on IT development in 2025, then identify relevant sources for content extraction.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 3):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='toolu_vrtx_01LYTvh7UB5TGAWFDsfZ97oj', arguments='{\"query\": \"Latest genai RAG techniques and their impact on IT development industry in 2025\"}', name='create_research_plan')]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 4):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='RESEARCH PLAN FOR: Latest genai RAG techniques and their impact on IT development industry in 2025\\nGenerated: 2025-08-06T16:27:08.515039\\n\\nPHASE 1: Source Discovery & Identification\\n- Use search_web_sources() to find relevant URLs across:\\n  * General web sources (Google Search)\\n  * Academic sources (Google Scholar)\\n  * News sources (Google News)\\n- Focus on recent, authoritative sources\\n\\nPHASE 2: Content Extraction & Analysis\\n- Use scrape_website() to extract content from discovered URLs\\n- Analyze content for key facts, insights, and data points\\n- Extract relevant quotes and statistics\\n\\nPHASE 3: Source Validation & Citation Management\\n- Verify source credibility and authority\\n- Check publication dates and author credentials\\n- Create properly formatted APA citations\\n- Identify any bias or limitations\\n\\nPHASE 4: Synthesis & Final Report\\n- Synthesize findings from all sources\\n- Create comprehensive analysis with evidence-based conclusions\\n- Structure report with executive summary, methodology, findings, and recommendations\\n\\nMETHODOLOGY: Web-first research using ScrapingDog APIs for real-time content discovery and extraction\\nREADY FOR IMPLEMENTATION', name='create_research_plan', call_id='toolu_vrtx_01LYTvh7UB5TGAWFDsfZ97oj', is_error=False)]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 5):\n",
      "--------------------------------------------------\n",
      "RESEARCH PLAN FOR: Latest genai RAG techniques and their impact on IT development industry in 2025\n",
      "Generated: 2025-08-06T16:27:08.515039\n",
      "\n",
      "PHASE 1: Source Discovery & Identification\n",
      "- Use search_web_sources() to find relevant URLs across:\n",
      "  * General web sources (Google Search)\n",
      "  * Academic sources (Google Scholar)\n",
      "  * News sources (Google News)\n",
      "- Focus on recent, authoritative sources\n",
      "\n",
      "PHASE 2: Content Extraction & Analysis\n",
      "- Use scrape_website() to extract content from discovered URLs\n",
      "- Analyze content for key facts, insights, and data points\n",
      "- Extract relevant quotes and statistics\n",
      "\n",
      "PHASE 3: Source Validation & Citation Management\n",
      "- Verify source credibility and authority\n",
      "- Check publication dates and author credentials\n",
      "- Create properly formatted APA citations\n",
      "- Identify any bias or limitations\n",
      "\n",
      "PHASE 4: Synthesis & Final Report\n",
      "- Synthesize findings from all sources\n",
      "- Create comprehensive analysis with evidence-based conclusions\n",
      "- Structure report with executive summary, methodology, findings, and recommendations\n",
      "\n",
      "METHODOLOGY: Web-first research using ScrapingDog APIs for real-time content discovery and extraction\n",
      "READY FOR IMPLEMENTATION\n",
      "\n",
      "\n",
      "[WebSearchAgent] (Message 6):\n",
      "--------------------------------------------------\n",
      "SEARCH RESULTS:\n",
      "\n",
      "I'll help you conduct comprehensive research on the latest GenAI RAG techniques and their impact on the IT development industry in 2025. Let me start by extracting content from key sources to gather the most current information.\n",
      "\n",
      "\n",
      "[WebSearchAgent] (Message 7):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='toolu_vrtx_01Mv6dQCiJs2XdJmwhPj8cjS', arguments='{\"url\": \"https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2024-gartner-hype-cycle\"}', name='scrape_website')]\n",
      "\n",
      "\n",
      "[WebSearchAgent] (Message 8):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='SCRAPED CONTENT FROM: https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2024-gartner-hype-cycle\\n\\nTITLE: 404 Page\\nDESCRIPTION: \\nCONTENT: 404 Page Sign In We‚Äôre sorry. We can‚Äôt seem to find the page you‚Äôve requested. We‚Äôd like to help you find what you‚Äôre looking for: Are you a client? Log in for resources. Client Login Explore our webinars Hear what expert analysts are talking about. View Webinars Are you looking for popular research and insights? Discover more. Read more Want to experience Gartner research in action? Join your peers at our live and virtual conferences. View Conferences Drive stronger performance on your mission-critical priorities. Become a Client Policies Privacy Policy Terms of Use Ombuds ¬©2025\\xa0Gartner, Inc. and/or its affiliates. All rights reserved.\\nSCRAPED AT: 2025-08-06T16:27:23.701037\\n\\nSTATUS: Content successfully extracted.', name='scrape_website', call_id='toolu_vrtx_01Mv6dQCiJs2XdJmwhPj8cjS', is_error=False)]\n",
      "\n",
      "\n",
      "[WebSearchAgent] (Message 9):\n",
      "--------------------------------------------------\n",
      "SCRAPED CONTENT FROM: https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2024-gartner-hype-cycle\n",
      "\n",
      "TITLE: 404 Page\n",
      "DESCRIPTION: \n",
      "CONTENT: 404 Page Sign In We‚Äôre sorry. We can‚Äôt seem to find the page you‚Äôve requested. We‚Äôd like to help you find what you‚Äôre looking for: Are you a client? Log in for resources. Client Login Explore our webinars Hear what expert analysts are talking about. View Webinars Are you looking for popular research and insights? Discover more. Read more Want to experience Gartner research in action? Join your peers at our live and virtual conferences. View Conferences Drive stronger performance on your mission-critical priorities. Become a Client Policies Privacy Policy Terms of Use Ombuds ¬©2025¬†Gartner, Inc. and/or its affiliates. All rights reserved.\n",
      "SCRAPED AT: 2025-08-06T16:27:23.701037\n",
      "\n",
      "STATUS: Content successfully extracted.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 10):\n",
      "--------------------------------------------------\n",
      "I see the Gartner URL returned a 404 error. Let me search for current and relevant sources on GenAI RAG techniques and their impact on IT development in 2025.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 11):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='toolu_vrtx_01JMefy7xPiXZVJgzYQffWns', arguments='{\"query\": \"RAG retrieval augmented generation techniques 2025 IT development impact\", \"include_academic\": true, \"include_news\": true}', name='search_web_sources')]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 12):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='SEARCH SUMMARY:\\n- Total URLs found: 0\\n- Search completed at: 2025-08-06T16:27:38.238089\\n- Next step: Use scrape_website() to extract content from relevant URLs', name='search_web_sources', call_id='toolu_vrtx_01JMefy7xPiXZVJgzYQffWns', is_error=False)]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 13):\n",
      "--------------------------------------------------\n",
      "SEARCH SUMMARY:\n",
      "- Total URLs found: 0\n",
      "- Search completed at: 2025-08-06T16:27:38.238089\n",
      "- Next step: Use scrape_website() to extract content from relevant URLs\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 14):\n",
      "--------------------------------------------------\n",
      "Let me try a more specific search for GenAI RAG developments:\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 15):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='toolu_vrtx_01G9UfQRU4JRsKGve161fzWW', arguments='{\"query\": \"\\\\\"generative AI\\\\\" \\\\\"retrieval augmented generation\\\\\" 2025 enterprise development\", \"include_academic\": true, \"include_news\": true}', name='search_web_sources')]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 16):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='SEARCH SUMMARY:\\n- Total URLs found: 0\\n- Search completed at: 2025-08-06T16:27:53.880384\\n- Next step: Use scrape_website() to extract content from relevant URLs', name='search_web_sources', call_id='toolu_vrtx_01G9UfQRU4JRsKGve161fzWW', is_error=False)]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 17):\n",
      "--------------------------------------------------\n",
      "SEARCH SUMMARY:\n",
      "- Total URLs found: 0\n",
      "- Search completed at: 2025-08-06T16:27:53.880384\n",
      "- Next step: Use scrape_website() to extract content from relevant URLs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with PROVEN best practices approach\n",
    "research_topic = \"Latest genai RAG techniques and their impact on IT development industry in 2025\"\n",
    "\n",
    "print(\"üöÄ Testing AutoGen Research System with PROVEN Best Practices\")\n",
    "# print(\"=\" * 70)\n",
    "# print(\"Based on research of working AutoGen v0.4 implementations:\")\n",
    "# print(\"‚úÖ Simple, context-based selector prompt\")\n",
    "# print(\"‚úÖ Clear agent descriptions\")\n",
    "# print(\"‚úÖ Flexible agent selection (allow_repeated_speaker=True)\")\n",
    "# print(\"‚úÖ RoundRobin fallback strategy\")\n",
    "# print(\"‚úÖ Reasonable termination conditions\")\n",
    "# print()\n",
    "# print(\"Expected behavior: Intelligent agent selection based on context\")\n",
    "# print(\"Fallback: If SelectorGroupChat fails, automatically use RoundRobin\")\n",
    "# print()\n",
    "\n",
    "success = await run_research(research_topic)\n",
    "\n",
    "print(f\"\\nüéØ Final Result: {'‚úÖ SUCCESS' if success else '‚ùå NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "if not success:\n",
    "    print(\"\\nüí° Next steps if this still doesn't work:\")\n",
    "    print(\"1. Try using GPT-4o instead of Claude 3.5 Sonnet for selection\")\n",
    "    print(\"2. Reduce max_messages to 8-10 to force quicker decisions\")\n",
    "    print(\"3. Consider using RoundRobin as primary approach\")\n",
    "    print(\"4. Check if API keys/models are working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with CONTEXT-AWARE selector prompt using history/roles/participants\n",
    "research_topic = \"Latest developments in quantum computing and their impact on cybersecurity\"\n",
    "\n",
    "print(\"üöÄ Testing AutoGen with CONTEXT-AWARE Selector Prompt\")\n",
    "print(\"=\" * 70)\n",
    "print(\"New approach uses AutoGen v0.4 template variables:\")\n",
    "print(\"‚úÖ {history} - Full conversation context for intelligent decisions\")\n",
    "print(\"‚úÖ {roles} - Agent descriptions and capabilities\")\n",
    "print(\"‚úÖ {participants} - Available agents for selection\")\n",
    "print(\"‚úÖ Context-aware workflow progression\")\n",
    "print()\n",
    "print(\"Expected behavior: Selector analyzes conversation and chooses appropriate agent\")\n",
    "print(\"No more rigid handoff signals - intelligent context-based selection\")\n",
    "print()\n",
    "\n",
    "success = await run_research(research_topic)\n",
    "\n",
    "print(f\"\\nüéØ Final Result: {'‚úÖ SUCCESS' if success else '‚ùå NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "if success:\n",
    "    print(\"üéâ The context-aware selector prompt is working!\")\n",
    "    print(\"The selector successfully used conversation history to choose appropriate agents.\")\n",
    "else:\n",
    "    print(\"\\nüí° If this still doesn't work, try these alternatives:\")\n",
    "    print(\"1. Use GPT-4 or Claude 4 Sonnet for the selector model (stronger reasoning)\")\n",
    "    print(\"2. Reduce max_messages to 8-10 for faster decisions\")\n",
    "    print(\"3. The RoundRobin fallback should still work reliably\")\n",
    "    print(\"4. Check that all agents have clear, descriptive descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ENFORCED handoff system\n",
    "research_topic = \"Latest developments in quantum computing and their impact on cybersecurity\"\n",
    "\n",
    "print(\"Testing ENFORCED HANDOFF AutoGen Research System\")\n",
    "print(\"This version uses aggressive handoff enforcement and prevents agent repetition\")\n",
    "print(\"\\nExpected workflow: Planning -> WebSearch -> Citation -> Finalize\")\n",
    "print(\"Each agent MUST hand off to the next agent exactly once\")\n",
    "\n",
    "# Create explicit task message that reinforces handoffs\n",
    "task_message = f\"\"\"URGENT RESEARCH REQUEST: {research_topic}\n",
    "\n",
    "MANDATORY WORKFLOW SEQUENCE (MUST FOLLOW EXACTLY):\n",
    "1. PlanningAgent: Create plan, identify sources, then say \"HANDOFF TO WEBSEARCH AGENT\"\n",
    "2. WebSearchAgent: Scrape content, extract findings, then say \"HANDOFF TO CITATION AGENT\" \n",
    "3. CitationAgent: Review sources, create citations, then say \"HANDOFF TO FINALIZE AGENT\"\n",
    "4. FinalizeAgent: Create final report starting with \"FINAL REPORT:\"\n",
    "\n",
    "CRITICAL: Each agent speaks exactly once and MUST use the handoff signal to pass control.\n",
    "This is a STRICT sequential workflow - no deviations allowed.\"\"\"\n",
    "\n",
    "print(f\"\\nStarting Research: {research_topic}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "message_count = 0\n",
    "agents_participated = set()\n",
    "has_final_report = False\n",
    "\n",
    "try:\n",
    "    async for message in agent_team.run_stream(task=task_message):\n",
    "        message_count += 1\n",
    "        \n",
    "        if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "            agent_name = message.source\n",
    "            agents_participated.add(agent_name)\n",
    "            \n",
    "            if \"FINAL REPORT:\" in message.content:\n",
    "                has_final_report = True\n",
    "            \n",
    "            print(f\"\\n[{agent_name}] (Message {message_count}):\")\n",
    "            print(\"-\" * 50)\n",
    "            print(message.content)\n",
    "            print()\n",
    "            \n",
    "            # Show handoff detection\n",
    "            if \"HANDOFF TO\" in message.content:\n",
    "                handoff_line = [line for line in message.content.split('\\n') if 'HANDOFF TO' in line]\n",
    "                if handoff_line:\n",
    "                    print(f\"üîÑ HANDOFF DETECTED: {handoff_line[0].strip()}\")\n",
    "            \n",
    "        elif str(type(message).__name__) == 'TaskResult':\n",
    "            print(f\"\\n[TASK RESULT]: Session completed after {message_count} messages\")\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Research error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Research Session Complete!\")\n",
    "print(f\"Total messages: {message_count}\")\n",
    "print(f\"Agents participated: {', '.join(sorted(agents_participated))}\")\n",
    "print(f\"Final report generated: {'Yes' if has_final_report else 'No'}\")\n",
    "\n",
    "if len(agents_participated) >= 3 and has_final_report:\n",
    "    print(\"SUCCESS: Research completed successfully!\")\n",
    "else:\n",
    "    print(\"PARTIAL: Research had some issues\")\n",
    "\n",
    "success = has_final_report and len(agents_participated) >= 3\n",
    "print(f\"\\nResearch Success: {success}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
