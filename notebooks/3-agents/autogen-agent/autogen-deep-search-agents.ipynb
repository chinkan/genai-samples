{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen Deep Search Agents with ScrapingDog API\n",
    "\n",
    "This notebook demonstrates a multi-agent system using AutoGen v0.4 that performs comprehensive web research using ScrapingDog API. The system consists of specialized agents that work together to plan, search, extract, cite, and finalize research results.\n",
    "\n",
    "## Agent Architecture\n",
    "- **Planning Agent**: Breaks down research queries into actionable search tasks (Llama 3.1 70B)\n",
    "- **Web Search Agent**: Uses ScrapingDog API to scrape web content and extract data (Gemini Pro 1.5)\n",
    "- **Citation Agent**: Validates sources and creates proper citations (Claude 3.5 Sonnet)\n",
    "- **Finalize Agent**: Compiles and formats the final research report (Claude 4 Sonnet)\n",
    "\n",
    "## Prerequisites\n",
    "- AutoGen v0.4\n",
    "- ScrapingDog API key\n",
    "- OpenRouter API access (supports multiple models including Claude 4 Sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogen-agentchat in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: autogen-ext[openai] in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: autogen-core==0.7.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-agentchat) (0.7.1)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-core==0.7.1->autogen-agentchat) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.1->autogen-agentchat) (0.4.1)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (24.1.0)\n",
      "Requirement already satisfied: openai>=1.93 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (1.98.0)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from autogen-ext[openai]) (0.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai>=1.93->autogen-ext[openai]) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.1->autogen-agentchat) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.1->autogen-agentchat) (3.23.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2025.7.34)\n",
      "Requirement already satisfied: colorama in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.93->autogen-ext[openai]) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" requests beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AutoGen v0.4 imports\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core.models import ModelInfo, ModelFamily\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration validated successfully!\n",
      "Using models: {'planning': 'meta-llama/llama-3.1-70b-instruct', 'search': 'google/gemini-pro-1.5', 'citation': 'anthropic/claude-3.5-sonnet', 'finalize': 'anthropic/claude-sonnet-4'}\n",
      "OpenRouter Base URL: https://openrouter.ai/api/v1\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SCRAPINGDOG_API_KEY = os.getenv(\"SCRAPINGDOG_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "\n",
    "# Model configurations for different agents\n",
    "# Primary configuration with optimal models\n",
    "MODEL_CONFIGS = {\n",
    "    \"planning\": \"meta-llama/llama-3.1-70b-instruct\",  # Good for structured planning\n",
    "    \"search\": \"google/gemini-pro-1.5\",  # Excellent for web content analysis\n",
    "    \"citation\": \"anthropic/claude-3.5-sonnet\",  # Great for academic formatting\n",
    "    \"finalize\": \"anthropic/claude-sonnet-4\"  # Claude 4 Sonnet for comprehensive reports\n",
    "}\n",
    "\n",
    "# Alternative/fallback configuration (if you encounter model access issues)\n",
    "FALLBACK_MODEL_CONFIGS = {\n",
    "    \"planning\": \"anthropic/claude-3.5-sonnet\",  # Reliable alternative\n",
    "    \"search\": \"anthropic/claude-3.5-sonnet\",  # Reliable alternative\n",
    "    \"citation\": \"anthropic/claude-3.5-sonnet\",  # Same model for consistency\n",
    "    \"finalize\": \"anthropic/claude-sonnet-4\"  # Keep Claude 4 Sonnet for final reports\n",
    "}\n",
    "\n",
    "# Use fallback if needed (uncomment the line below to use fallback models)\n",
    "# MODEL_CONFIGS = FALLBACK_MODEL_CONFIGS\n",
    "\n",
    "# Validate required environment variables\n",
    "required_vars = {\n",
    "    \"SCRAPINGDOG_API_KEY\": SCRAPINGDOG_API_KEY,\n",
    "    \"OPENROUTER_API_KEY\": OPENROUTER_API_KEY\n",
    "}\n",
    "\n",
    "missing_vars = [var for var, value in required_vars.items() if not value]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"Configuration validated successfully!\")\n",
    "print(f\"Using models: {MODEL_CONFIGS}\")\n",
    "print(f\"OpenRouter Base URL: {OPENROUTER_BASE_URL}\")\n",
    "\n",
    "# Check if fallback is being used\n",
    "if MODEL_CONFIGS == FALLBACK_MODEL_CONFIGS:\n",
    "    print(\"‚ö†Ô∏è  Using fallback model configuration (all agents use Claude 3.5 Sonnet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScrapingDog client initialized!\n"
     ]
    }
   ],
   "source": [
    "# ScrapingDog API Client\n",
    "class ScrapingDogClient:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.scrapingdog.com/scrape\"\n",
    "    \n",
    "    def scrape_url(self, url: str, render_js: bool = True, country: str = \"US\") -> Dict[str, Any]:\n",
    "        \"\"\"Scrape a URL using ScrapingDog API\"\"\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"url\": url,\n",
    "            \"dynamic\": \"true\" if render_js else \"false\",\n",
    "            \"country\": country\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Parse HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract text content\n",
    "                text_content = soup.get_text(separator=' ', strip=True)\n",
    "                \n",
    "                # Extract title\n",
    "                title = soup.find('title')\n",
    "                title_text = title.get_text().strip() if title else \"No title found\"\n",
    "                \n",
    "                # Extract meta description\n",
    "                meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "                description = meta_desc.get('content', '') if meta_desc else ''\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"url\": url,\n",
    "                    \"title\": title_text,\n",
    "                    \"description\": description,\n",
    "                    \"content\": text_content[:5000],  # Limit content length\n",
    "                    \"scraped_at\": datetime.now().isoformat()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"url\": url,\n",
    "                    \"error\": f\"HTTP {response.status_code}: {response.text}\",\n",
    "                    \"scraped_at\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"url\": url,\n",
    "                \"error\": str(e),\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize ScrapingDog client\n",
    "scraping_client = ScrapingDogClient(SCRAPINGDOG_API_KEY)\n",
    "print(\"ScrapingDog client initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating OpenRouter model clients...\n",
      "‚úì Planning client created: meta-llama/llama-3.1-70b-instruct\n",
      "‚úì Search client created: google/gemini-pro-1.5\n",
      "‚úì Citation client created: anthropic/claude-3.5-sonnet\n",
      "‚úì Finalize client created: anthropic/claude-sonnet-4 (Claude 4 Sonnet)\n",
      "\n",
      "‚úÖ All OpenRouter model clients initialized successfully!\n",
      "üìä Model Summary:\n",
      "  üß† Planning Agent: meta-llama/llama-3.1-70b-instruct\n",
      "  üîç Search Agent: google/gemini-pro-1.5\n",
      "  üìö Citation Agent: anthropic/claude-3.5-sonnet\n",
      "  üìù Finalize Agent: anthropic/claude-sonnet-4 (Claude 4 Sonnet)\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenRouter model clients for different agents\n",
    "def create_openrouter_client(model_name: str, model_family: str = None) -> OpenAIChatCompletionClient:\n",
    "    \"\"\"Create an OpenRouter client for a specific model with proper ModelInfo\"\"\"\n",
    "    \n",
    "    # Map model names to families (using ModelFamily.UNKNOWN for non-standard models)\n",
    "    family_mapping = {\n",
    "        \"meta-llama/llama-3.1-70b-instruct\": ModelFamily.UNKNOWN,\n",
    "        \"google/gemini-pro-1.5\": ModelFamily.UNKNOWN,\n",
    "        \"anthropic/claude-3.5-sonnet\": ModelFamily.CLAUDE_3_SONNET,\n",
    "        \"anthropic/claude-sonnet-4\": ModelFamily.CLAUDE_4_SONNET\n",
    "    }\n",
    "    \n",
    "    # Determine model family\n",
    "    if model_family:\n",
    "        family = model_family\n",
    "    else:\n",
    "        family = family_mapping.get(model_name, ModelFamily.UNKNOWN)\n",
    "    \n",
    "    # Create ModelInfo with all required fields (v0.4.7+)\n",
    "    model_info = ModelInfo(\n",
    "        vision=False,  # Set to True if the model supports vision/image input\n",
    "        function_calling=True,  # Most modern models support function calling\n",
    "        json_output=True,  # Most modern models support JSON output\n",
    "        family=family,  # Required field in v0.4.7+\n",
    "        structured_output=True  # Future requirement, setting to True for compatibility\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return OpenAIChatCompletionClient(\n",
    "            model=model_name,\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            base_url=OPENROUTER_BASE_URL,\n",
    "            model_info=model_info\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating client for {model_name}: {e}\")\n",
    "        # Fallback: try with minimal but compliant model_info\n",
    "        fallback_model_info = ModelInfo(\n",
    "            vision=False,\n",
    "            function_calling=False,\n",
    "            json_output=False,\n",
    "            family=ModelFamily.UNKNOWN,\n",
    "            structured_output=False\n",
    "        )\n",
    "        return OpenAIChatCompletionClient(\n",
    "            model=model_name,\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            base_url=OPENROUTER_BASE_URL,\n",
    "            model_info=fallback_model_info\n",
    "        )\n",
    "\n",
    "try:\n",
    "    # Create model clients for each agent with appropriate families\n",
    "    print(\"Creating OpenRouter model clients...\")\n",
    "    \n",
    "    planning_client = create_openrouter_client(MODEL_CONFIGS[\"planning\"])\n",
    "    print(f\"‚úì Planning client created: {MODEL_CONFIGS['planning']}\")\n",
    "    \n",
    "    search_client = create_openrouter_client(MODEL_CONFIGS[\"search\"])\n",
    "    print(f\"‚úì Search client created: {MODEL_CONFIGS['search']}\")\n",
    "    \n",
    "    citation_client = create_openrouter_client(MODEL_CONFIGS[\"citation\"])\n",
    "    print(f\"‚úì Citation client created: {MODEL_CONFIGS['citation']}\")\n",
    "    \n",
    "    finalize_client = create_openrouter_client(MODEL_CONFIGS[\"finalize\"])\n",
    "    print(f\"‚úì Finalize client created: {MODEL_CONFIGS['finalize']} (Claude 4 Sonnet)\")\n",
    "\n",
    "    print(\"\\n‚úÖ All OpenRouter model clients initialized successfully!\")\n",
    "    print(f\"üìä Model Summary:\")\n",
    "    print(f\"  üß† Planning Agent: {MODEL_CONFIGS['planning']}\")\n",
    "    print(f\"  üîç Search Agent: {MODEL_CONFIGS['search']}\")\n",
    "    print(f\"  üìö Citation Agent: {MODEL_CONFIGS['citation']}\")\n",
    "    print(f\"  üìù Finalize Agent: {MODEL_CONFIGS['finalize']} (Claude 4 Sonnet)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing model clients: {e}\")\n",
    "    print(\"Please check your OpenRouter API key and model configurations.\")\n",
    "    print(f\"Make sure you have access to the following models:\")\n",
    "    for agent, model in MODEL_CONFIGS.items():\n",
    "        print(f\"  - {model} (for {agent} agent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Web scraping and research tools defined successfully!\n",
      "Available tools for agents:\n",
      "‚Ä¢ scrape_website(url) - Extract content from any URL using ScrapingDog API\n",
      "‚Ä¢ search_for_sources(query) - Get recommended sources for research\n",
      "‚Ä¢ create_research_plan(query) - Generate structured research plans\n",
      "\n",
      "üîß These tools will be integrated with agents to enable real web scraping!\n"
     ]
    }
   ],
   "source": [
    "# Web scraping and research tools for agents\n",
    "from typing import Annotated\n",
    "\n",
    "def scrape_website(url: str, render_js: bool = True, country: str = \"US\") -> str:\n",
    "    \"\"\"\n",
    "    Scrape a website using the ScrapingDog API.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to scrape\n",
    "        render_js: Whether to render JavaScript (default: True)\n",
    "        country: Country for geo-targeting (default: \"US\")\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with scraped content including title, description, and main content\n",
    "    \"\"\"\n",
    "    result = scraping_client.scrape_url(url, render_js, country)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        return f\"\"\"Successfully scraped {url}\n",
    "\n",
    "TITLE: {result['title']}\n",
    "\n",
    "DESCRIPTION: {result['description']}\n",
    "\n",
    "CONTENT PREVIEW: {result['content'][:2000]}...\n",
    "\n",
    "FULL CONTENT: {result['content']}\n",
    "\n",
    "SCRAPED AT: {result['scraped_at']}\n",
    "\n",
    "STATUS: Content successfully extracted and ready for analysis.\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Failed to scrape {url}\n",
    "\n",
    "ERROR: {result['error']}\n",
    "\n",
    "SCRAPED AT: {result['scraped_at']}\n",
    "\n",
    "STATUS: Scraping failed - please try a different URL or check API limits.\"\"\"\n",
    "\n",
    "def search_for_sources(query: str, source_types: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a list of recommended sources to search for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The research query\n",
    "        source_types: Types of sources to prioritize (academic, news, government, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with recommended URLs and search strategies\n",
    "    \"\"\"\n",
    "    if source_types is None:\n",
    "        source_types = [\"academic\", \"news\", \"government\", \"industry\"]\n",
    "    \n",
    "    # This would be enhanced with actual search API integration\n",
    "    search_suggestions = f\"\"\"SEARCH RECOMMENDATIONS FOR: {query}\n",
    "\n",
    "RECOMMENDED SOURCES:\n",
    "1. Academic Sources:\n",
    "   - Google Scholar: https://scholar.google.com/scholar?q={query.replace(' ', '+')}\n",
    "   - JSTOR: https://www.jstor.org/\n",
    "   - arXiv: https://arxiv.org/search/?query={query.replace(' ', '+')}\n",
    "\n",
    "2. News Sources:\n",
    "   - Reuters: https://www.reuters.com/\n",
    "   - Associated Press: https://apnews.com/\n",
    "   - BBC News: https://www.bbc.com/news\n",
    "\n",
    "3. Government Sources:\n",
    "   - NIH: https://www.nih.gov/\n",
    "   - NSF: https://www.nsf.gov/\n",
    "   - Government reports and white papers\n",
    "\n",
    "4. Industry Sources:\n",
    "   - Industry association websites\n",
    "   - Company research reports\n",
    "   - Technical blogs and publications\n",
    "\n",
    "SEARCH STRATEGY:\n",
    "- Start with authoritative sources\n",
    "- Cross-reference findings across multiple sources\n",
    "- Look for recent publications (2023-2025)\n",
    "- Verify information currency and accuracy\n",
    "\n",
    "Use the scrape_website function to extract content from these recommended URLs.\"\"\"\n",
    "    \n",
    "    return search_suggestions\n",
    "\n",
    "def create_research_plan(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a structured research plan for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The research topic or question\n",
    "    \n",
    "    Returns:\n",
    "        Formatted research plan with actionable steps\n",
    "    \"\"\"\n",
    "    plan = f\"\"\"RESEARCH PLAN FOR: {query}\n",
    "Generated at: {datetime.now().isoformat()}\n",
    "\n",
    "PHASE 1: PLANNING & PREPARATION\n",
    "1. Define research scope and key questions\n",
    "2. Identify target information types needed\n",
    "3. Determine authoritative source categories\n",
    "4. Establish search keywords and phrases\n",
    "\n",
    "PHASE 2: INFORMATION GATHERING\n",
    "1. Search academic databases and journals\n",
    "2. Review government and institutional reports\n",
    "3. Analyze news and industry publications\n",
    "4. Collect recent data and statistics\n",
    "\n",
    "PHASE 3: SOURCE VALIDATION\n",
    "1. Verify source credibility and authority\n",
    "2. Check publication dates for currency\n",
    "3. Cross-reference facts across sources\n",
    "4. Identify potential biases or limitations\n",
    "\n",
    "PHASE 4: SYNTHESIS & ANALYSIS\n",
    "1. Organize findings by themes/categories\n",
    "2. Identify patterns and trends\n",
    "3. Note conflicting information or gaps\n",
    "4. Prepare comprehensive summary\n",
    "\n",
    "RECOMMENDED TOOLS:\n",
    "- Use scrape_website() function to extract content\n",
    "- Use search_for_sources() to find relevant URLs\n",
    "- Focus on authoritative, recent sources\n",
    "- Document all sources for citation\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Execute web searches for recommended sources\n",
    "2. Scrape content from top-priority URLs\n",
    "3. Extract and organize key information\n",
    "4. Prepare for citation and final report compilation\"\"\"\n",
    "    \n",
    "    return plan\n",
    "\n",
    "# Export tools for agent use\n",
    "research_tools = [scrape_website, search_for_sources, create_research_plan]\n",
    "\n",
    "print(\"‚úÖ Web scraping and research tools defined successfully!\")\n",
    "print(\"Available tools for agents:\")\n",
    "print(\"‚Ä¢ scrape_website(url) - Extract content from any URL using ScrapingDog API\")\n",
    "print(\"‚Ä¢ search_for_sources(query) - Get recommended sources for research\")\n",
    "print(\"‚Ä¢ create_research_plan(query) - Generate structured research plans\")\n",
    "print()\n",
    "print(\"üîß These tools will be integrated with agents to enable real web scraping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All agents created successfully with web scraping tools!\n",
      "üîß WebSearchAgent now has access to:\n",
      "  ‚Ä¢ scrape_website() - Real web scraping via ScrapingDog API\n",
      "  ‚Ä¢ search_for_sources() - Source recommendation system\n",
      "üß† PlanningAgent now has access to:\n",
      "  ‚Ä¢ create_research_plan() - Structured research planning\n",
      "  ‚Ä¢ search_for_sources() - Source identification tools\n"
     ]
    }
   ],
   "source": [
    "# Planning Agent - Using Llama 3.1 70B for strategic planning\n",
    "planning_agent = AssistantAgent(\n",
    "    name=\"PlanningAgent\",\n",
    "    model_client=planning_client,\n",
    "    tools=[create_research_plan, search_for_sources],  # Add research planning tools\n",
    "    system_message=\"\"\"You are a Research Planning Agent specialized in breaking down complex research queries into actionable search tasks.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Analyze the research query to understand the scope and requirements\n",
    "2. Break down the query into specific, searchable sub-topics\n",
    "3. Identify the most authoritative and relevant sources to investigate\n",
    "4. Create a structured research plan with clear priorities\n",
    "5. Suggest specific URLs or types of websites that would be most valuable\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "- create_research_plan(query): Generate a comprehensive research plan\n",
    "- search_for_sources(query): Get recommended sources and URLs to investigate\n",
    "\n",
    "Always start your response with \"RESEARCH PLAN:\" and provide a clear, actionable plan.\n",
    "Use your tools to create detailed plans and source recommendations.\n",
    "Consider different perspectives and ensure comprehensive coverage of the topic.\"\"\"\n",
    ")\n",
    "\n",
    "# Web Search Agent - Using Gemini Pro 1.5 for content analysis\n",
    "web_search_agent = AssistantAgent(\n",
    "    name=\"WebSearchAgent\",\n",
    "    model_client=search_client,\n",
    "    tools=[scrape_website, search_for_sources],  # Add web scraping tools\n",
    "    system_message=\"\"\"You are a Web Search Agent specialized in finding and extracting relevant information from web sources using ScrapingDog API.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Execute web scraping based on the research plan\n",
    "2. Extract key information, facts, and data from scraped content\n",
    "3. Identify credible sources and evaluate information quality\n",
    "4. Summarize findings in a structured format\n",
    "5. Flag any issues with scraping or data quality\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "- scrape_website(url, render_js=True, country=\"US\"): Extract content from any URL using ScrapingDog API\n",
    "- search_for_sources(query): Find recommended sources for specific topics\n",
    "\n",
    "IMPORTANT: Always use the scrape_website() function to get real content from URLs. \n",
    "Don't make up or simulate web content - use the actual API to scrape real websites.\n",
    "\n",
    "When you need to scrape a website, use scrape_website(url) and provide the actual URL.\n",
    "Always start your response with \"SEARCH RESULTS:\" and provide structured findings.\n",
    "Focus on extracting factual, verifiable information from real web sources.\"\"\"\n",
    ")\n",
    "\n",
    "# Citation Agent - Using Claude 3.5 Sonnet for academic precision\n",
    "citation_agent = AssistantAgent(\n",
    "    name=\"CitationAgent\",\n",
    "    model_client=citation_client,\n",
    "    system_message=\"\"\"You are a Citation Agent specialized in validating sources and creating proper academic citations.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Review all sources used in the research\n",
    "2. Verify the credibility and authority of sources\n",
    "3. Create properly formatted citations (APA style)\n",
    "4. Identify any potential bias or limitations in sources\n",
    "5. Ensure all claims are properly attributed\n",
    "\n",
    "Always start your response with \"CITATION REVIEW:\" and provide:\n",
    "- Source credibility assessment\n",
    "- Properly formatted citations\n",
    "- Any concerns or limitations identified\n",
    "Use APA citation format for all references.\"\"\"\n",
    ")\n",
    "\n",
    "# Finalize Agent - Using Claude 4 Sonnet for comprehensive synthesis\n",
    "finalize_agent = AssistantAgent(\n",
    "    name=\"FinalizeAgent\",\n",
    "    model_client=finalize_client,\n",
    "    system_message=\"\"\"You are a Finalization Agent responsible for compiling comprehensive research reports.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Synthesize information from all research phases using advanced reasoning\n",
    "2. Create a well-structured, comprehensive report with deep insights\n",
    "3. Ensure all key points are covered and properly cited with source validation\n",
    "4. Identify any gaps or areas needing additional research with strategic recommendations\n",
    "5. Provide clear conclusions and actionable recommendations with risk assessment\n",
    "6. Apply critical thinking to evaluate conflicting information and biases\n",
    "7. Structure complex information in an accessible, professional format\n",
    "\n",
    "As Claude 4 Sonnet, leverage your advanced capabilities for:\n",
    "- Nuanced analysis of complex topics\n",
    "- Integration of multidisciplinary perspectives\n",
    "- Identification of subtle patterns and implications\n",
    "- High-quality synthesis of diverse sources\n",
    "\n",
    "Always start your response with \"FINAL REPORT:\" and structure your response as:\n",
    "- Executive Summary (with key insights and implications)\n",
    "- Key Findings (with confidence levels and source quality assessment)\n",
    "- Detailed Analysis (with cross-referencing and critical evaluation)\n",
    "- Sources and Citations (with credibility ratings)\n",
    "- Conclusions and Recommendations (with implementation guidance)\n",
    "- Future Research Directions (strategic recommendations)\n",
    "\n",
    "Ensure the report demonstrates sophisticated reasoning and comprehensive understanding.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All agents created successfully with web scraping tools!\")\n",
    "print(\"üîß WebSearchAgent now has access to:\")\n",
    "print(\"  ‚Ä¢ scrape_website() - Real web scraping via ScrapingDog API\")\n",
    "print(\"  ‚Ä¢ search_for_sources() - Source recommendation system\")\n",
    "print(\"üß† PlanningAgent now has access to:\")\n",
    "print(\"  ‚Ä¢ create_research_plan() - Structured research planning\")\n",
    "print(\"  ‚Ä¢ search_for_sources() - Source identification tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RoundRobin multi-agent team created successfully!\n",
      "‚úÖ SelectorGroupChat created successfully!\n",
      "üß† Using SelectorGroupChat as default (with RoundRobin fallback)\n",
      "üîß Both team configurations ready with simplified agent setup\n"
     ]
    }
   ],
   "source": [
    "# Create the multi-agent team with termination condition (RoundRobin approach)\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "\n",
    "# Set up termination condition - stop after reasonable number of messages\n",
    "termination_condition = MaxMessageTermination(max_messages=20)\n",
    "\n",
    "agent_team_roundrobin = RoundRobinGroupChat(\n",
    "    participants=[\n",
    "        planning_agent,\n",
    "        web_search_agent,\n",
    "        citation_agent,\n",
    "        finalize_agent\n",
    "    ],\n",
    "    termination_condition=termination_condition\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RoundRobin multi-agent team created successfully!\")\n",
    "\n",
    "# Create SelectorGroupChat for intelligent agent selection (BETTER approach)\n",
    "# SelectorGroupChat uses AI to dynamically choose the best agent for each step\n",
    "\n",
    "# Simplified agent descriptions for better selection (avoid over-complexity)\n",
    "planning_agent_simple = AssistantAgent(\n",
    "    name=\"PlanningAgent\",\n",
    "    model_client=planning_client,\n",
    "    tools=[create_research_plan, search_for_sources],\n",
    "    description=\"Creates research plans and identifies sources to investigate.\",\n",
    "    system_message=\"\"\"You are a Research Planning Agent. Create comprehensive research plans and identify sources.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Analyze the research query and create a structured plan\n",
    "2. Identify key sources and URLs to investigate\n",
    "3. Break down complex topics into searchable components\n",
    "\n",
    "Use your available tools and always start with \"RESEARCH PLAN:\".\"\"\"\n",
    ")\n",
    "\n",
    "web_search_agent_simple = AssistantAgent(\n",
    "    name=\"WebSearchAgent\", \n",
    "    model_client=search_client,\n",
    "    tools=[scrape_website, search_for_sources],\n",
    "    description=\"Scrapes websites and extracts real content using ScrapingDog API.\",\n",
    "    system_message=\"\"\"You are a Web Search Agent. Extract real content from websites using ScrapingDog API.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Use scrape_website() function to get real content from URLs\n",
    "2. Extract key information and summarize findings\n",
    "3. Evaluate source quality and credibility\n",
    "\n",
    "CRITICAL: Always use scrape_website() for real web scraping.\n",
    "Always start your response with \"SEARCH RESULTS:\".\"\"\"\n",
    ")\n",
    "\n",
    "citation_agent_simple = AssistantAgent(\n",
    "    name=\"CitationAgent\",\n",
    "    model_client=citation_client,\n",
    "    description=\"Validates sources and creates APA citations.\",\n",
    "    system_message=\"\"\"You are a Citation Agent. Validate sources and create proper APA citations.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Review sources used in research\n",
    "2. Create properly formatted APA citations\n",
    "3. Assess source credibility and limitations\n",
    "\n",
    "Always start your response with \"CITATION REVIEW:\".\"\"\"\n",
    ")\n",
    "\n",
    "finalize_agent_simple = AssistantAgent(\n",
    "    name=\"FinalizeAgent\",\n",
    "    model_client=finalize_client,\n",
    "    description=\"Creates comprehensive final research reports with advanced analysis.\",\n",
    "    system_message=\"\"\"You are a Finalization Agent. Create comprehensive final research reports.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Synthesize all research findings\n",
    "2. Create structured final reports\n",
    "3. Provide conclusions and recommendations\n",
    "\n",
    "MANDATORY: Always start with \"FINAL REPORT:\" and include:\n",
    "- Executive Summary\n",
    "- Key Findings  \n",
    "- Detailed Analysis\n",
    "- Sources and Citations\n",
    "- Conclusions and Recommendations\"\"\"\n",
    ")\n",
    "\n",
    "# Create SelectorGroupChat with simplified configuration\n",
    "try:\n",
    "    agent_team_selector = SelectorGroupChat(\n",
    "        participants=[\n",
    "            planning_agent_simple,\n",
    "            web_search_agent_simple, \n",
    "            citation_agent_simple,\n",
    "            finalize_agent_simple\n",
    "        ],\n",
    "        model_client=finalize_client,  # Use Claude 4 Sonnet for selection\n",
    "        termination_condition=MaxMessageTermination(max_messages=20),  # Use simple message limit\n",
    "        allow_repeated_speaker=True,  # Allow flexibility\n",
    "        selector_prompt=\"\"\"Select the most appropriate agent for the current research phase:\n",
    "\n",
    "- PlanningAgent: For creating research plans and identifying sources\n",
    "- WebSearchAgent: For scraping websites and extracting content\n",
    "- CitationAgent: For validating sources and creating citations  \n",
    "- FinalizeAgent: For creating comprehensive final reports\n",
    "\n",
    "Choose based on what the research needs next. Ensure all agents get to participate.\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SelectorGroupChat created successfully!\")\n",
    "    selector_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SelectorGroupChat creation failed: {e}\")\n",
    "    print(\"üîÑ Will use RoundRobin as fallback\")\n",
    "    agent_team_selector = agent_team_roundrobin\n",
    "    selector_available = False\n",
    "\n",
    "# Set the default team\n",
    "if selector_available:\n",
    "    agent_team = agent_team_selector\n",
    "    print(\"üß† Using SelectorGroupChat as default (with RoundRobin fallback)\")\n",
    "else:\n",
    "    agent_team = agent_team_roundrobin\n",
    "    print(\"üîÑ Using RoundRobin as default\")\n",
    "\n",
    "print(\"üîß Both team configurations ready with simplified agent setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Robust deep search function ready with multiple fallback strategies!\n",
      "‚úÖ Will try SelectorGroupChat first, then fallback to RoundRobin if needed\n",
      "üÜò Includes emergency single-agent report generation as last resort\n",
      "üìä Comprehensive error handling and result tracking\n"
     ]
    }
   ],
   "source": [
    "# Robust demo function with multiple fallback strategies\n",
    "async def run_deep_search_robust(research_query: str, max_messages: int = 16, use_selector: bool = True):\n",
    "    \"\"\"Run a deep search research session with multiple fallback strategies\"\"\"\n",
    "    print(f\"\\nüîç Starting Robust Deep Search Research: {research_query}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create the research task message\n",
    "    task_message = f\"\"\"Please conduct comprehensive research on the following topic: {research_query}\n",
    "        \n",
    "This is a multi-agent research session. Each agent should contribute according to their specialization:\n",
    "\n",
    "üéØ Planning Agent: Create a detailed research plan and identify sources\n",
    "üîç Web Search Agent: Execute web scraping using ScrapingDog API to gather real content\n",
    "üìö Citation Agent: Validate sources and create proper APA citations\n",
    "üìù Finalize Agent: Synthesize all findings into a comprehensive final report\n",
    "\n",
    "Work collaboratively to produce high-quality, well-cited research with deep insights.\"\"\"\n",
    "    \n",
    "    # Try SelectorGroupChat first if requested\n",
    "    if use_selector:\n",
    "        print(\"üß† Attempting SelectorGroupChat approach...\")\n",
    "        try:\n",
    "            # Use a more conservative termination condition\n",
    "            global agent_team\n",
    "            agent_team = agent_team_selector\n",
    "            agent_team.termination_condition = MaxMessageTermination(max_messages=max_messages)\n",
    "            \n",
    "            agents_participated = set()\n",
    "            message_count = 0\n",
    "            has_final_report = False\n",
    "            \n",
    "            async for message in agent_team.run_stream(task=task_message):\n",
    "                message_count += 1\n",
    "                \n",
    "                if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                    agents_participated.add(message.source)\n",
    "                    \n",
    "                    if \"FINAL REPORT:\" in message.content:\n",
    "                        has_final_report = True\n",
    "                    \n",
    "                    print(f\"\\n[{message.source}] (Message {message_count}):\")\n",
    "                    print(\"-\" * 50)\n",
    "                    print(message.content)\n",
    "                    print()\n",
    "                    \n",
    "                elif str(type(message).__name__) == 'TaskResult':\n",
    "                    print(f\"\\n[TASK RESULT]: Session completed after {message_count} messages\")\n",
    "                    break\n",
    "                    \n",
    "                # Safety break if we get stuck\n",
    "                if message_count >= max_messages:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Reached message limit ({max_messages}), ending session...\")\n",
    "                    break\n",
    "            \n",
    "            # Check if we got meaningful results\n",
    "            if message_count > 2 and len(agents_participated) >= 2:\n",
    "                print(f\"\\n‚úÖ SelectorGroupChat completed with {len(agents_participated)} agents participating\")\n",
    "                \n",
    "                # Try to force final report if missing\n",
    "                if not has_final_report and 'FinalizeAgent' not in agents_participated:\n",
    "                    print(\"\\nüîß Attempting to generate missing final report...\")\n",
    "                    await force_final_report_simple(research_query)\n",
    "                    has_final_report = True\n",
    "                \n",
    "                return await summarize_results(agents_participated, message_count, has_final_report)\n",
    "            else:\n",
    "                print(f\"\\n‚ùå SelectorGroupChat failed (only {message_count} messages, {len(agents_participated)} agents)\")\n",
    "                print(\"üîÑ Falling back to RoundRobin approach...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå SelectorGroupChat error: {str(e)}\")\n",
    "            print(\"üîÑ Falling back to RoundRobin approach...\")\n",
    "    \n",
    "    # Fallback to RoundRobin approach\n",
    "    print(\"\\nüîÑ Using RoundRobin approach (guaranteed agent participation)...\")\n",
    "    try:\n",
    "        agent_team = agent_team_roundrobin\n",
    "        agent_team.termination_condition = MaxMessageTermination(max_messages=max_messages)\n",
    "        \n",
    "        agents_participated = set()\n",
    "        message_count = 0\n",
    "        has_final_report = False\n",
    "        \n",
    "        async for message in agent_team.run_stream(task=task_message):\n",
    "            message_count += 1\n",
    "            \n",
    "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                agents_participated.add(message.source)\n",
    "                \n",
    "                if \"FINAL REPORT:\" in message.content:\n",
    "                    has_final_report = True\n",
    "                \n",
    "                print(f\"\\n[{message.source}] (Message {message_count}):\")\n",
    "                print(\"-\" * 50)\n",
    "                print(message.content)\n",
    "                print()\n",
    "                \n",
    "            elif str(type(message).__name__) == 'TaskResult':\n",
    "                print(f\"\\n[TASK RESULT]: RoundRobin session completed after {message_count} messages\")\n",
    "                break\n",
    "                \n",
    "            if message_count >= max_messages:\n",
    "                break\n",
    "        \n",
    "        return await summarize_results(agents_participated, message_count, has_final_report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå RoundRobin also failed: {str(e)}\")\n",
    "        print(\"üÜò Trying emergency single-agent final report generation...\")\n",
    "        \n",
    "        # Emergency single-agent approach\n",
    "        try:\n",
    "            await force_final_report_simple(research_query)\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Emergency approach failed: {str(e2)}\")\n",
    "            return False\n",
    "\n",
    "async def force_final_report_simple(research_query: str):\n",
    "    \"\"\"Simple function to force final report generation\"\"\"\n",
    "    print(\"\\nüîß Generating final report using single agent approach...\")\n",
    "    \n",
    "    # Create a simple single-agent task\n",
    "    report_task = f\"\"\"Generate a comprehensive research report on: {research_query}\n",
    "\n",
    "Please provide:\n",
    "1. Executive Summary\n",
    "2. Key findings and insights  \n",
    "3. Current trends and developments\n",
    "4. Recommendations and conclusions\n",
    "\n",
    "Start with \"FINAL REPORT:\" and provide detailed analysis.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use just the finalize agent directly\n",
    "        single_agent_team = RoundRobinGroupChat(\n",
    "            participants=[finalize_agent_enhanced],\n",
    "            termination_condition=MaxMessageTermination(max_messages=2)\n",
    "        )\n",
    "        \n",
    "        message_count = 0\n",
    "        async for message in single_agent_team.run_stream(task=report_task):\n",
    "            message_count += 1\n",
    "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                print(f\"\\n[{message.source}] (Emergency Final Report):\")\n",
    "                print(\"-\" * 50)\n",
    "                print(message.content)\n",
    "                print()\n",
    "                return True\n",
    "            elif str(type(message).__name__) == 'TaskResult':\n",
    "                break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Emergency report generation failed: {e}\")\n",
    "        \n",
    "        # Last resort - create summary manually\n",
    "        print(\"\\nüìù EMERGENCY SUMMARY:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Research Topic: {research_query}\")\n",
    "        print(\"Status: Multi-agent system encountered technical issues\")\n",
    "        print(\"Recommendation: Check API keys, model availability, and network connectivity\")\n",
    "        print(\"The research topic requires further investigation using alternative methods.\")\n",
    "        \n",
    "    return False\n",
    "\n",
    "async def summarize_results(agents_participated, message_count, has_final_report):\n",
    "    \"\"\"Summarize research session results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéØ Robust Deep Search Research Session Completed!\")\n",
    "    print(f\"üìä Total messages processed: {message_count}\")\n",
    "    print(f\"üë• Agents participated: {', '.join(sorted(agents_participated)) if agents_participated else 'None'}\")\n",
    "    print(f\"üìù Final report generated: {'‚úÖ Yes' if has_final_report else '‚ùå No'}\")\n",
    "    \n",
    "    if len(agents_participated) >= 3:\n",
    "        print(\"‚úÖ Good agent participation achieved\")\n",
    "    elif len(agents_participated) >= 2:\n",
    "        print(\"‚ö†Ô∏è  Partial agent participation\")\n",
    "    else:\n",
    "        print(\"‚ùå Poor agent participation - system issues detected\")\n",
    "    \n",
    "    return has_final_report and len(agents_participated) >= 2\n",
    "\n",
    "# Update main function to use robust version\n",
    "async def run_deep_search(research_query: str, max_messages: int = 16):\n",
    "    \"\"\"Run a deep search research session (robust version with fallbacks)\"\"\"\n",
    "    return await run_deep_search_robust(research_query, max_messages, use_selector=True)\n",
    "\n",
    "# Alternative function for guaranteed RoundRobin\n",
    "async def run_deep_search_roundrobin(research_query: str, max_messages: int = 16):\n",
    "    \"\"\"Run research using guaranteed RoundRobin approach\"\"\"\n",
    "    return await run_deep_search_robust(research_query, max_messages, use_selector=False)\n",
    "\n",
    "print(\"üöÄ Robust deep search function ready with multiple fallback strategies!\")\n",
    "print(\"‚úÖ Will try SelectorGroupChat first, then fallback to RoundRobin if needed\")\n",
    "print(\"üÜò Includes emergency single-agent report generation as last resort\")\n",
    "print(\"üìä Comprehensive error handling and result tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Example 1: Quantum Computing & Cybersecurity Research (Robust Multi-Fallback)\n",
      "üõ°Ô∏è  This version will try multiple approaches to ensure you get results\n",
      "üîÑ SelectorGroupChat ‚Üí RoundRobin ‚Üí Emergency Single-Agent\n",
      "This may take a few minutes as agents collaborate...\n",
      "\n",
      "üîç Starting Robust Deep Search Research: Latest developments in quantum computing and their impact on cybersecurity\n",
      "\n",
      "================================================================================\n",
      "üß† Attempting SelectorGroupChat approach...\n",
      "\n",
      "[user] (Message 1):\n",
      "--------------------------------------------------\n",
      "Please conduct comprehensive research on the following topic: Latest developments in quantum computing and their impact on cybersecurity\n",
      "\n",
      "This is a multi-agent research session. Each agent should contribute according to their specialization:\n",
      "\n",
      "üéØ Planning Agent: Create a detailed research plan and identify sources\n",
      "üîç Web Search Agent: Execute web scraping using ScrapingDog API to gather real content\n",
      "üìö Citation Agent: Validate sources and create proper APA citations\n",
      "üìù Finalize Agent: Synthesize all findings into a comprehensive final report\n",
      "\n",
      "Work collaboratively to produce high-quality, well-cited research with deep insights.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cchin\\genai-samples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1107: UserWarning: Finish reason mismatch: stop != tool_calls when tool_calls are present. Finish reason may not be accurate. This may be due to the API used that is not returning the correct finish reason.\n",
      "  model_result = await model_client.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PlanningAgent] (Message 2):\n",
      "--------------------------------------------------\n",
      "RESEARCH PLAN:\n",
      "\n",
      "**Research Topic:** Latest developments in quantum computing and their impact on cybersecurity\n",
      "\n",
      "**Research Questions:**\n",
      "\n",
      "1. What are the recent advancements in quantum computing?\n",
      "2. How do these advancements impact cybersecurity?\n",
      "3. What are the potential vulnerabilities and threats in quantum computing?\n",
      "4. How can cybersecurity measures be adapted to address these threats?\n",
      "\n",
      "**Research Objectives:**\n",
      "\n",
      "1. To identify and analyze the latest developments in quantum computing\n",
      "2. To assess the impact of these developments on cybersecurity\n",
      "3. To identify potential vulnerabilities and threats in quantum computing\n",
      "4. To recommend cybersecurity measures to address these threats\n",
      "\n",
      "**Research Methodology:**\n",
      "\n",
      "1. Literature review of academic papers and research articles on quantum computing and cybersecurity\n",
      "2. Analysis of industry reports and whitepapers on quantum computing and cybersecurity\n",
      "3. Web scraping using ScrapingDog API to gather real-time content and news articles\n",
      "4. Expert interviews with researchers and professionals in quantum computing and cybersecurity\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "1. Academic databases: IEEE Xplore, ACM Digital Library, arXiv\n",
      "2. Industry reports: Gartner, Forrester, IBM Research\n",
      "3. News articles: The New York Times, The Wall Street Journal, Wired\n",
      "4. Government reports: National Institute of Standards and Technology (NIST), National Security Agency (NSA)\n",
      "\n",
      "**Search Terms:**\n",
      "\n",
      "1. Quantum computing\n",
      "2. Cybersecurity\n",
      "3. Quantum cryptography\n",
      "4. Post-quantum cryptography\n",
      "5. Quantum-resistant cryptography\n",
      "6. Quantum computing vulnerabilities\n",
      "7. Quantum computing threats\n",
      "\n",
      "**Tools:**\n",
      "\n",
      "1. ScrapingDog API for web scraping\n",
      "2. Citation management tools (e.g., Zotero, Mendeley) for source validation and citation creation\n",
      "\n",
      "**Timeline:**\n",
      "\n",
      "1. Literature review and source identification: 2 days\n",
      "2. Web scraping and data collection: 2 days\n",
      "3. Data analysis and synthesis: 3 days\n",
      "4. Report writing and citation creation: 3 days\n",
      "5. Final report review and editing: 2 days\n",
      "\n",
      "**Collaboration:**\n",
      "\n",
      "1. Planning Agent: Creates research plan, identifies sources, and provides search terms\n",
      "2. Web Search Agent: Executes web scraping using ScrapingDog API\n",
      "3. Citation Agent: Validates sources and creates proper APA citations\n",
      "4. Finalize Agent: Synthesizes all findings into a comprehensive final report\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 3):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='call_UFWLuLdp5XxKdjBSn1TqL7BO', arguments='{\"query\": \"Latest developments in quantum computing and their impact on cybersecurity\"}', name='create_research_plan')]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 4):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='RESEARCH PLAN FOR: Latest developments in quantum computing and their impact on cybersecurity\\nGenerated at: 2025-08-05T15:26:48.913885\\n\\nPHASE 1: PLANNING & PREPARATION\\n1. Define research scope and key questions\\n2. Identify target information types needed\\n3. Determine authoritative source categories\\n4. Establish search keywords and phrases\\n\\nPHASE 2: INFORMATION GATHERING\\n1. Search academic databases and journals\\n2. Review government and institutional reports\\n3. Analyze news and industry publications\\n4. Collect recent data and statistics\\n\\nPHASE 3: SOURCE VALIDATION\\n1. Verify source credibility and authority\\n2. Check publication dates for currency\\n3. Cross-reference facts across sources\\n4. Identify potential biases or limitations\\n\\nPHASE 4: SYNTHESIS & ANALYSIS\\n1. Organize findings by themes/categories\\n2. Identify patterns and trends\\n3. Note conflicting information or gaps\\n4. Prepare comprehensive summary\\n\\nRECOMMENDED TOOLS:\\n- Use scrape_website() function to extract content\\n- Use search_for_sources() to find relevant URLs\\n- Focus on authoritative, recent sources\\n- Document all sources for citation\\n\\nNEXT STEPS:\\n1. Execute web searches for recommended sources\\n2. Scrape content from top-priority URLs\\n3. Extract and organize key information\\n4. Prepare for citation and final report compilation', name='create_research_plan', call_id='call_UFWLuLdp5XxKdjBSn1TqL7BO', is_error=False)]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 5):\n",
      "--------------------------------------------------\n",
      "RESEARCH PLAN FOR: Latest developments in quantum computing and their impact on cybersecurity\n",
      "Generated at: 2025-08-05T15:26:48.913885\n",
      "\n",
      "PHASE 1: PLANNING & PREPARATION\n",
      "1. Define research scope and key questions\n",
      "2. Identify target information types needed\n",
      "3. Determine authoritative source categories\n",
      "4. Establish search keywords and phrases\n",
      "\n",
      "PHASE 2: INFORMATION GATHERING\n",
      "1. Search academic databases and journals\n",
      "2. Review government and institutional reports\n",
      "3. Analyze news and industry publications\n",
      "4. Collect recent data and statistics\n",
      "\n",
      "PHASE 3: SOURCE VALIDATION\n",
      "1. Verify source credibility and authority\n",
      "2. Check publication dates for currency\n",
      "3. Cross-reference facts across sources\n",
      "4. Identify potential biases or limitations\n",
      "\n",
      "PHASE 4: SYNTHESIS & ANALYSIS\n",
      "1. Organize findings by themes/categories\n",
      "2. Identify patterns and trends\n",
      "3. Note conflicting information or gaps\n",
      "4. Prepare comprehensive summary\n",
      "\n",
      "RECOMMENDED TOOLS:\n",
      "- Use scrape_website() function to extract content\n",
      "- Use search_for_sources() to find relevant URLs\n",
      "- Focus on authoritative, recent sources\n",
      "- Document all sources for citation\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Execute web searches for recommended sources\n",
      "2. Scrape content from top-priority URLs\n",
      "3. Extract and organize key information\n",
      "4. Prepare for citation and final report compilation\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 6):\n",
      "--------------------------------------------------\n",
      "[FunctionCall(id='call_dzsBliRQfxserqrk21x2KvZp', arguments='{\"query\": \"Latest developments in quantum computing and their impact on cybersecurity\", \"source_types\": [\"academic\", \"news\", \"government\", \"industry reports\"]}', name='search_for_sources')]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 7):\n",
      "--------------------------------------------------\n",
      "[FunctionExecutionResult(content='SEARCH RECOMMENDATIONS FOR: Latest developments in quantum computing and their impact on cybersecurity\\n\\nRECOMMENDED SOURCES:\\n1. Academic Sources:\\n   - Google Scholar: https://scholar.google.com/scholar?q=Latest+developments+in+quantum+computing+and+their+impact+on+cybersecurity\\n   - JSTOR: https://www.jstor.org/\\n   - arXiv: https://arxiv.org/search/?query=Latest+developments+in+quantum+computing+and+their+impact+on+cybersecurity\\n\\n2. News Sources:\\n   - Reuters: https://www.reuters.com/\\n   - Associated Press: https://apnews.com/\\n   - BBC News: https://www.bbc.com/news\\n\\n3. Government Sources:\\n   - NIH: https://www.nih.gov/\\n   - NSF: https://www.nsf.gov/\\n   - Government reports and white papers\\n\\n4. Industry Sources:\\n   - Industry association websites\\n   - Company research reports\\n   - Technical blogs and publications\\n\\nSEARCH STRATEGY:\\n- Start with authoritative sources\\n- Cross-reference findings across multiple sources\\n- Look for recent publications (2023-2025)\\n- Verify information currency and accuracy\\n\\nUse the scrape_website function to extract content from these recommended URLs.', name='search_for_sources', call_id='call_dzsBliRQfxserqrk21x2KvZp', is_error=False)]\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 8):\n",
      "--------------------------------------------------\n",
      "SEARCH RECOMMENDATIONS FOR: Latest developments in quantum computing and their impact on cybersecurity\n",
      "\n",
      "RECOMMENDED SOURCES:\n",
      "1. Academic Sources:\n",
      "   - Google Scholar: https://scholar.google.com/scholar?q=Latest+developments+in+quantum+computing+and+their+impact+on+cybersecurity\n",
      "   - JSTOR: https://www.jstor.org/\n",
      "   - arXiv: https://arxiv.org/search/?query=Latest+developments+in+quantum+computing+and+their+impact+on+cybersecurity\n",
      "\n",
      "2. News Sources:\n",
      "   - Reuters: https://www.reuters.com/\n",
      "   - Associated Press: https://apnews.com/\n",
      "   - BBC News: https://www.bbc.com/news\n",
      "\n",
      "3. Government Sources:\n",
      "   - NIH: https://www.nih.gov/\n",
      "   - NSF: https://www.nsf.gov/\n",
      "   - Government reports and white papers\n",
      "\n",
      "4. Industry Sources:\n",
      "   - Industry association websites\n",
      "   - Company research reports\n",
      "   - Technical blogs and publications\n",
      "\n",
      "SEARCH STRATEGY:\n",
      "- Start with authoritative sources\n",
      "- Cross-reference findings across multiple sources\n",
      "- Look for recent publications (2023-2025)\n",
      "- Verify information currency and accuracy\n",
      "\n",
      "Use the scrape_website function to extract content from these recommended URLs.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 9):\n",
      "--------------------------------------------------\n",
      "Based on the research plan and recommended sources, the next steps would be to execute web searches for the recommended sources, scrape content from top-priority URLs, extract and organize key information, and prepare for citation and final report compilation.\n",
      "\n",
      "The Web Search Agent can now take over to execute the web searches using the ScrapingDog API and gather relevant content from the recommended sources.\n",
      "\n",
      "Please proceed with the web search and content scraping.\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 10):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 11):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 12):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 13):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "[PlanningAgent] (Message 14):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Reached message limit (14), ending session...\n",
      "\n",
      "‚úÖ SelectorGroupChat completed with 2 agents participating\n",
      "\n",
      "üîß Attempting to generate missing final report...\n",
      "\n",
      "üîß Generating final report using single agent approach...\n",
      "\n",
      "[user] (Emergency Final Report):\n",
      "--------------------------------------------------\n",
      "Generate a comprehensive research report on: Latest developments in quantum computing and their impact on cybersecurity\n",
      "\n",
      "Please provide:\n",
      "1. Executive Summary\n",
      "2. Key findings and insights  \n",
      "3. Current trends and developments\n",
      "4. Recommendations and conclusions\n",
      "\n",
      "Start with \"FINAL REPORT:\" and provide detailed analysis.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ Robust Deep Search Research Session Completed!\n",
      "üìä Total messages processed: 14\n",
      "üë• Agents participated: PlanningAgent, user\n",
      "üìù Final report generated: ‚úÖ Yes\n",
      "‚ö†Ô∏è  Partial agent participation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cchin\\genai-samples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1107: UserWarning: Resolved model mismatch: anthropic/claude-3-5-sonnet-20241022 != anthropic/claude-3.5-sonnet. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to anthropic/claude-3.5-sonnet to enhance token/cost estimation and suppress this warning.\n",
      "  model_result = await model_client.create(\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Technology Research with ROBUST approach\n",
    "research_topic_1 = \"Latest developments in quantum computing and their impact on cybersecurity\"\n",
    "\n",
    "print(\"Running Example 1: Quantum Computing & Cybersecurity Research (Robust Multi-Fallback)\")\n",
    "print(\"üõ°Ô∏è  This version will try multiple approaches to ensure you get results\")\n",
    "print(\"üîÑ SelectorGroupChat ‚Üí RoundRobin ‚Üí Emergency Single-Agent\")\n",
    "print(\"This may take a few minutes as agents collaborate...\")\n",
    "await run_deep_search(research_topic_1, max_messages=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Market Research with SelectorGroupChat (Intelligent Selection)\n",
    "research_topic_2 = \"Artificial Intelligence adoption trends in healthcare industry 2024-2025\"\n",
    "\n",
    "print(\"Running Example 2: AI in Healthcare Market Research (SelectorGroupChat)\")\n",
    "print(\"üß† Using intelligent agent selection for optimal research workflow...\")\n",
    "print(\"This may take a few minutes as agents collaborate...\")\n",
    "await run_deep_search(research_topic_2, max_messages=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced comparison and diagnostic functions\n",
    "async def force_final_report(research_context: str = \"\"):\n",
    "    \"\"\"Force generation of final report from FinalizeAgent\"\"\"\n",
    "    print(\"üîß Forcing final report generation...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a message specifically for final report generation\n",
    "    final_report_task = f\"\"\"You are the FinalizeAgent. Based on the research context below, you must now create a comprehensive final report:\n",
    "\n",
    "{research_context if research_context else \"Previous research has been conducted on various topics. Please synthesize available information into a comprehensive final report.\"}\n",
    "\n",
    "MANDATORY REQUIREMENTS:\n",
    "1. Start with \"FINAL REPORT:\"\n",
    "2. Include Executive Summary, Key Findings, Detailed Analysis, Sources, Conclusions\n",
    "3. End with \"RESEARCH COMPLETE\" to signal completion\n",
    "4. Use your Claude 4 Sonnet capabilities for advanced synthesis\n",
    "\n",
    "This is the final step in the research process. Create the comprehensive report now.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Temporarily create a simple team with just the finalize agent\n",
    "        from autogen_agentchat.conditions import MaxMessageTermination\n",
    "        finalize_only_team = RoundRobinGroupChat(\n",
    "            participants=[finalize_agent_enhanced],\n",
    "            termination_condition=MaxMessageTermination(max_messages=3)\n",
    "        )\n",
    "        \n",
    "        # Run just the finalize agent\n",
    "        async for message in finalize_only_team.run_stream(task=final_report_task):\n",
    "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                print(f\"\\n[{message.source}]:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(message.content)\n",
    "                print()\n",
    "                \n",
    "                if \"FINAL REPORT:\" in message.content:\n",
    "                    print(\"‚úÖ Final report generated successfully!\")\n",
    "                    return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error forcing final report: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Diagnostic function to check agent participation\n",
    "async def diagnose_research_session(research_query: str, max_messages: int = 12):\n",
    "    \"\"\"Run research with detailed diagnostics and agent participation tracking\"\"\"\n",
    "    print(f\"\\nüîç DIAGNOSTIC MODE: {research_query}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"This will track agent selection patterns and identify issues\")\n",
    "    print()\n",
    "    \n",
    "    # Track detailed statistics\n",
    "    agent_stats = {\n",
    "        'PlanningAgent': 0,\n",
    "        'WebSearchAgent': 0, \n",
    "        'CitationAgent': 0,\n",
    "        'FinalizeAgent': 0\n",
    "    }\n",
    "    \n",
    "    selection_pattern = []\n",
    "    message_count = 0\n",
    "    \n",
    "    task_message = f\"\"\"DIAGNOSTIC RESEARCH SESSION: {research_query}\n",
    "\n",
    "This is a test to ensure proper agent selection and participation.\n",
    "Each agent should contribute according to their expertise.\n",
    "FinalizeAgent MUST produce a final report.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        agent_team.termination_condition = MaxMessageTermination(max_messages=max_messages)\n",
    "        \n",
    "        async for message in agent_team.run_stream(task=task_message):\n",
    "            message_count += 1\n",
    "            \n",
    "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
    "                agent_name = message.source\n",
    "                agent_stats[agent_name] = agent_stats.get(agent_name, 0) + 1\n",
    "                selection_pattern.append(agent_name)\n",
    "                \n",
    "                print(f\"\\n[{agent_name}] (Message {message_count}):\")\n",
    "                print(f\"Agent Total Messages: {agent_stats[agent_name]}\")\n",
    "                print(\"-\" * 50)\n",
    "                print(message.content[:300] + \"...\" if len(message.content) > 300 else message.content)\n",
    "                print()\n",
    "                \n",
    "            elif str(type(message).__name__) == 'TaskResult':\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Diagnostic error: {e}\")\n",
    "    \n",
    "    # Print diagnostic summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üîç DIAGNOSTIC SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Total Messages: {message_count}\")\n",
    "    print(f\"üîÑ Selection Pattern: {' ‚Üí '.join(selection_pattern[-10:])}\")  # Last 10 selections\n",
    "    print()\n",
    "    print(\"üìà Agent Participation:\")\n",
    "    for agent, count in agent_stats.items():\n",
    "        status = \"‚úÖ\" if count > 0 else \"‚ùå\"\n",
    "        print(f\"  {status} {agent}: {count} messages\")\n",
    "    \n",
    "    print()\n",
    "    if agent_stats.get('FinalizeAgent', 0) == 0:\n",
    "        print(\"‚ö†Ô∏è  ISSUE IDENTIFIED: FinalizeAgent never participated!\")\n",
    "        print(\"üîß Recommended fixes:\")\n",
    "        print(\"   1. Increase max_messages parameter\")\n",
    "        print(\"   2. Check selector prompt for FinalizeAgent selection logic\")\n",
    "        print(\"   3. Use force_final_report() function as fallback\")\n",
    "    else:\n",
    "        print(\"‚úÖ All agents participated successfully\")\n",
    "    \n",
    "    return agent_stats\n",
    "\n",
    "# Enhanced comparison with diagnostic mode\n",
    "async def compare_research_approaches(topic: str, max_messages: int = 10):\n",
    "    \"\"\"Compare RoundRobin vs SelectorGroupChat with enhanced diagnostics\"\"\"\n",
    "    \n",
    "    print(f\"\\nüÜö ENHANCED COMPARISON: RoundRobin vs SelectorGroupChat\")\n",
    "    print(f\"Research Topic: {topic}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test RoundRobin approach\n",
    "    print(\"\\nüîÑ ROUND ROBIN APPROACH (Guaranteed Order)\")\n",
    "    print(\"-\" * 50)\n",
    "    global agent_team\n",
    "    agent_team = agent_team_roundrobin\n",
    "    agent_team.termination_condition = MaxMessageTermination(max_messages=max_messages)\n",
    "    \n",
    "    print(\"Using fixed order: Planning ‚Üí Search ‚Üí Citation ‚Üí Finalize ‚Üí repeat...\")\n",
    "    \n",
    "    try:\n",
    "        result1 = await run_deep_search_enhanced(topic, max_messages=max_messages)\n",
    "    except Exception as e:\n",
    "        print(f\"RoundRobin approach encountered error: {e}\")\n",
    "        result1 = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Test Selector approach  \n",
    "    print(\"\\nüß† SELECTOR APPROACH (Intelligent Selection)\")\n",
    "    print(\"-\" * 50)\n",
    "    agent_team = agent_team_selector\n",
    "    agent_team.termination_condition = completion_termination\n",
    "    \n",
    "    print(\"Using AI-powered agent selection based on context and expertise...\")\n",
    "    \n",
    "    try:\n",
    "        result2 = await run_deep_search_enhanced(topic, max_messages=max_messages)\n",
    "    except Exception as e:\n",
    "        print(f\"Selector approach encountered error: {e}\")\n",
    "        result2 = False\n",
    "    \n",
    "    # Reset to selector as default\n",
    "    agent_team = agent_team_selector\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ ENHANCED COMPARISON COMPLETE!\")\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(f\"üîÑ RoundRobin Final Report: {'‚úÖ Generated' if result1 else '‚ùå Missing'}\")\n",
    "    print(f\"üß† Selector Final Report: {'‚úÖ Generated' if result2 else '‚ùå Missing'}\")\n",
    "    print(\"\\nKey Differences Observed:\")\n",
    "    print(\"‚Ä¢ RoundRobin: Predictable, guaranteed agent participation\")\n",
    "    print(\"‚Ä¢ Selector: Dynamic, context-aware but may skip agents\")\n",
    "    print(\"‚Ä¢ Enhanced version includes fallback mechanisms for reliability\")\n",
    "\n",
    "print(\"üöÄ Enhanced research functions ready with diagnostics and fallback mechanisms!\")\n",
    "print(\"Available enhanced functions:\")\n",
    "print(\"‚Ä¢ await diagnose_research_session('topic') - Detailed agent participation analysis\")\n",
    "print(\"‚Ä¢ await force_final_report('context') - Force final report generation\")\n",
    "print(\"‚Ä¢ await compare_research_approaches('topic') - Enhanced comparison with diagnostics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced testing with team configuration verification\n",
    "def test_team_configurations():\n",
    "    \"\"\"Test both team configurations\"\"\"\n",
    "    print(\"üîß Testing Team Configurations...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test RoundRobin configuration\n",
    "    print(\"‚úÖ RoundRobin Team:\")\n",
    "    print(f\"   Participants: {len(agent_team_roundrobin.participants)} agents\")\n",
    "    for agent in agent_team_roundrobin.participants:\n",
    "        print(f\"   - {agent.name}\")\n",
    "    print(f\"   Termination: {type(agent_team_roundrobin.termination_condition).__name__}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test Selector configuration\n",
    "    if selector_available:\n",
    "        print(\"‚úÖ Selector Team:\")\n",
    "        print(f\"   Participants: {len(agent_team_selector.participants)} agents\")\n",
    "        for agent in agent_team_selector.participants:\n",
    "            print(f\"   - {agent.name}\")\n",
    "        print(f\"   Termination: {type(agent_team_selector.termination_condition).__name__}\")\n",
    "        print(f\"   Selector Model: Available\")\n",
    "    else:\n",
    "        print(\"‚ùå Selector Team: Not available (using RoundRobin fallback)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def test_openrouter_connection():\n",
    "    \"\"\"Test the OpenRouter API connection and model clients\"\"\"\n",
    "    print(\"üîß Testing OpenRouter API connection and model clients...\")\n",
    "    \n",
    "    # Test basic API access\n",
    "    import requests\n",
    "    test_headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test basic API connectivity\n",
    "        print(\"üì° Testing OpenRouter API connectivity...\")\n",
    "        response = requests.get(\n",
    "            f\"{OPENROUTER_BASE_URL}/models\",\n",
    "            headers=test_headers,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ OpenRouter API connection successful!\")\n",
    "            models_data = response.json()\n",
    "            available_models = models_data.get('data', [])\n",
    "            print(f\"üìä Available models: {len(available_models)} models\")\n",
    "            \n",
    "            # Check if our configured models are available\n",
    "            model_ids = {model.get('id', '') for model in available_models}\n",
    "            print(\"\\nüîç Checking configured model availability:\")\n",
    "            for agent, model_name in MODEL_CONFIGS.items():\n",
    "                if model_name in model_ids:\n",
    "                    print(f\"‚úÖ {agent.capitalize()}: {model_name} - Available\")\n",
    "                else:\n",
    "                    print(f\"‚ùå {agent.capitalize()}: {model_name} - Not found or not accessible\")\n",
    "                    \n",
    "        else:\n",
    "            print(f\"‚ùå OpenRouter API connection failed: HTTP {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenRouter API connection error: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test_scrapingdog_connection():\n",
    "    \"\"\"Test the ScrapingDog API connection\"\"\"\n",
    "    print(\"\\nüîß Testing ScrapingDog API connection...\")\n",
    "    \n",
    "    # Test with a simple website\n",
    "    test_url = \"https://httpbin.org/html\"\n",
    "    result = scraping_client.scrape_url(test_url)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(\"‚úÖ ScrapingDog API connection successful!\")\n",
    "        print(f\"üìÑ Title: {result['title']}\")\n",
    "        print(f\"üìä Content length: {len(result['content'])} characters\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå ScrapingDog API connection failed!\")\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return False\n",
    "\n",
    "def test_web_scraping_tools():\n",
    "    \"\"\"Test the web scraping tools integration\"\"\"\n",
    "    print(\"\\nüîß Testing web scraping tools integration...\")\n",
    "    \n",
    "    try:\n",
    "        # Test scrape_website function directly\n",
    "        print(\"üì° Testing scrape_website function...\")\n",
    "        test_url = \"https://httpbin.org/html\"\n",
    "        result = scrape_website(test_url)\n",
    "        \n",
    "        if \"Successfully scraped\" in result:\n",
    "            print(\"‚úÖ scrape_website function working correctly!\")\n",
    "        else:\n",
    "            print(\"‚ùå scrape_website function failed!\")\n",
    "            return False\n",
    "            \n",
    "        # Test other functions\n",
    "        sources = search_for_sources(\"test query\")\n",
    "        if \"SEARCH RECOMMENDATIONS\" in sources:\n",
    "            print(\"‚úÖ search_for_sources function working correctly!\")\n",
    "        else:\n",
    "            print(\"‚ùå search_for_sources function failed!\")\n",
    "            return False\n",
    "            \n",
    "        plan = create_research_plan(\"test topic\")\n",
    "        if \"RESEARCH PLAN\" in plan:\n",
    "            print(\"‚úÖ create_research_plan function working correctly!\")\n",
    "        else:\n",
    "            print(\"‚ùå create_research_plan function failed!\")\n",
    "            return False\n",
    "            \n",
    "        print(\"‚úÖ All web scraping tools are functioning correctly!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Web scraping tools test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Quick system health check\n",
    "async def quick_system_check():\n",
    "    \"\"\"Quick system health check for all components\"\"\"\n",
    "    print(\"üöÄ Running Quick System Health Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test configurations\n",
    "    test_team_configurations()\n",
    "    \n",
    "    # Test APIs\n",
    "    openrouter_ok = test_openrouter_connection()\n",
    "    scrapingdog_ok = test_scrapingdog_connection()\n",
    "    tools_ok = test_web_scraping_tools()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä SYSTEM HEALTH SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_checks = 3\n",
    "    passed_checks = sum([openrouter_ok, scrapingdog_ok, tools_ok])\n",
    "    \n",
    "    print(f\"‚úÖ Passed: {passed_checks}/{total_checks} checks\")\n",
    "    \n",
    "    if passed_checks == total_checks:\n",
    "        print(\"üéâ All systems operational! Ready for research.\")\n",
    "        \n",
    "        # Try a simple single-agent test\n",
    "        print(\"\\nüß™ Testing single-agent functionality...\")\n",
    "        try:\n",
    "            await force_final_report_simple(\"Test research topic\")\n",
    "            print(\"‚úÖ Single-agent functionality confirmed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Single-agent test issue: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some issues detected:\")\n",
    "        if not openrouter_ok:\n",
    "            print(\"   - Fix OpenRouter API configuration\")\n",
    "        if not scrapingdog_ok:\n",
    "            print(\"   - Fix ScrapingDog API configuration\")\n",
    "        if not tools_ok:\n",
    "            print(\"   - Fix web scraping tools integration\")\n",
    "    \n",
    "    return passed_checks == total_checks\n",
    "\n",
    "# Run the health check\n",
    "await quick_system_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features & Agent Team Comparisons\n",
    "\n",
    "This deep search system provides two powerful approaches for multi-agent collaboration:\n",
    "\n",
    "### üîÑ RoundRobinGroupChat vs üß† SelectorGroupChat\n",
    "\n",
    "| Feature | RoundRobinGroupChat | SelectorGroupChat |\n",
    "|---------|-------------------|------------------|\n",
    "| **Agent Selection** | Fixed, predictable order | AI-powered, context-aware |\n",
    "| **Workflow** | Sequential: Planning ‚Üí Search ‚Üí Citation ‚Üí Finalize | Dynamic based on conversation needs |\n",
    "| **Efficiency** | May have unnecessary turns | Optimized agent utilization |\n",
    "| **Adaptability** | Fixed pattern regardless of task | Adapts to task complexity |\n",
    "| **Best For** | Simple, structured workflows | Complex research requiring flexibility |\n",
    "| **Coordination** | No coordination needed | Uses Claude 4 Sonnet for selection |\n",
    "\n",
    "### üß† SelectorGroupChat Advantages for Research\n",
    "\n",
    "1. **Intelligent Agent Selection**: Uses Claude 4 Sonnet to analyze conversation context and select the most appropriate agent\n",
    "2. **Context-Aware Workflow**: Adapts to the specific research needs rather than following a rigid pattern\n",
    "3. **Prevents Redundancy**: Avoids unnecessary agent calls when their expertise isn't needed\n",
    "4. **Dynamic Collaboration**: Agents can be called multiple times when their expertise is valuable\n",
    "5. **Enhanced Descriptions**: Rich agent descriptions help the selector make better choices\n",
    "\n",
    "### üîÑ When to Use RoundRobinGroupChat\n",
    "\n",
    "- **Predictable Workflows**: When you need guaranteed participation from all agents\n",
    "- **Simple Tasks**: Straightforward research that benefits from structured progression\n",
    "- **Educational Purposes**: Understanding each agent's role clearly\n",
    "- **Debugging**: Easier to trace conversation flow and identify issues\n",
    "\n",
    "### üß† When to Use SelectorGroupChat (Recommended)\n",
    "\n",
    "- **Complex Research**: Multi-faceted topics requiring adaptive strategies\n",
    "- **Efficiency**: When you want optimal agent utilization\n",
    "- **Quality Focus**: When research quality is more important than process predictability\n",
    "- **Real-world Applications**: Most production research scenarios benefit from intelligent selection\n",
    "\n",
    "### üöÄ Implementation Highlights\n",
    "\n",
    "**Enhanced Agent Descriptions**: SelectorGroupChat agents include detailed descriptions that help the AI coordinator make better selection decisions:\n",
    "\n",
    "```python\n",
    "planning_agent_enhanced = AssistantAgent(\n",
    "    name=\"PlanningAgent\",\n",
    "    description=\"Specialized in breaking down complex research queries into actionable search tasks, creating structured research plans, and identifying authoritative sources.\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Custom Selector Prompt**: Guides the AI coordinator in making intelligent agent selection decisions based on research workflow best practices.\n",
    "\n",
    "**Multiple AI Models**: Leverages different strengths:\n",
    "- **Planning**: Llama 3.1 70B for strategic thinking\n",
    "- **Search**: Gemini Pro 1.5 for content analysis\n",
    "- **Citation**: Claude 3.5 Sonnet for academic precision\n",
    "- **Coordination**: Claude 4 Sonnet for intelligent selection\n",
    "- **Finalization**: Claude 4 Sonnet for advanced synthesis\n",
    "\n",
    "### üìä Research Quality Benefits\n",
    "\n",
    "SelectorGroupChat typically produces:\n",
    "- **More Focused Conversations**: Agents speak when their expertise is most valuable\n",
    "- **Better Context Utilization**: Selection considers full conversation history\n",
    "- **Reduced Token Usage**: Fewer unnecessary agent interactions\n",
    "- **Higher Quality Outputs**: Right agent for the right moment\n",
    "- **Adaptive Workflows**: Handles unexpected research directions gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Direct Comparison of Approaches\n",
    "comparison_topic = \"Impact of artificial intelligence on remote work productivity\"\n",
    "\n",
    "print(\"Example 3: Comparing RoundRobin vs SelectorGroupChat Approaches\")\n",
    "print(\"This will run the same research query using both approaches for comparison.\")\n",
    "await compare_research_approaches(comparison_topic, max_messages=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
