{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# React Agent Evaluation with DeepEval\n",
    "\n",
    "This notebook demonstrates how to evaluate a React agent using the DeepEval framework. We'll use the React agent from the langchain-agent example and evaluate its performance using various metrics including:\n",
    "\n",
    "- Answer Relevancy\n",
    "- Faithfulness \n",
    "- Contextual Relevancy\n",
    "- Citation accuracy\n",
    "\n",
    "The evaluation will be performed on a set of questions loaded from a CSV file, and results will be saved back to the file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepeval in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: langchain in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (0.3.28)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: azure-search-documents in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (11.5.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (3.12.15)\n",
      "Requirement already satisfied: anthropic in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.60.0)\n",
      "Requirement already satisfied: click<8.3.0,>=8.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (8.2.1)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.28.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.74.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: ollama in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.5.1)\n",
      "Requirement already satisfied: openai in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.98.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (3.2.0)\n",
      "Requirement already satisfied: posthog<4.0.0,>=3.23.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (3.25.0)\n",
      "Requirement already satisfied: pyfiglet in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.0.3)\n",
      "Requirement already satisfied: pytest in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (1.1.0)\n",
      "Requirement already satisfied: pytest-repeat in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.9.4)\n",
      "Requirement already satisfied: pytest-rerunfailures<13.0,>=12.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (12.0)\n",
      "Requirement already satisfied: pytest-xdist in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (2.32.4)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (14.1.0)\n",
      "Requirement already satisfied: sentry-sdk in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (2.34.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (80.9.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (8.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.16.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from deepeval) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from click<8.3.0,>=8.0.0->deepeval) (0.4.6)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.11.7)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (6.31.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.57b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain) (0.4.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain) (2.0.42)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from aiohttp->deepeval) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openai->deepeval) (0.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.7.34)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.6.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp312-cp312-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from azure-search-documents) (1.35.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pytest->deepeval) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pytest->deepeval) (1.6.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from portalocker->deepeval) (311)\n",
      "Requirement already satisfied: execnet>=2.1 in c:\\users\\cchin\\genai-samples\\.venv\\lib\\site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "Downloading langgraph-0.6.3-py3-none-any.whl (152 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.6.3-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
      "Downloading ormsgpack-1.10.0-cp312-cp312-win_amd64.whl (121 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   ---------------------------------------- 6/6 [langgraph]\n",
      "\n",
      "Successfully installed langgraph-0.6.3 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.3 langgraph-sdk-0.2.0 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install deepeval langchain langchain-community langchain-openai langgraph azure-search-documents pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    GEval\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents._generated.models import QueryType\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure OpenAI and Search Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Initialize Azure Search client\n",
    "search_client = SearchIndexClient(\n",
    "    endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), \n",
    "    credential=AzureKeyCredential(key=os.getenv(\"AZURE_AI_SEARCH_API_KEY\"))\n",
    ")\n",
    "index_details = search_client.get_index(os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"))\n",
    "\n",
    "print(\"‚úÖ Azure clients initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Search Tool and React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_index(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    This tool searches the Azure Search index for the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: The search results with filename, content, score, and metadata.\n",
    "    \"\"\"\n",
    "    client = search_client.get_search_client(index_name=os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"))\n",
    "    search_fields = [field.name for field in index_details.fields]\n",
    "    search_results = client.search(\n",
    "        search_text=query, \n",
    "        query_type=QueryType.SIMPLE,\n",
    "        top=10,\n",
    "        select=search_fields,\n",
    "    )\n",
    "    documents = [{\n",
    "        \"filename\": i.get('title', ''),\n",
    "        \"content\": i.get('chunk', ''),\n",
    "        \"score\": i.get('@search.score', 0),\n",
    "        \"metadata\": i.get('metadata', {})\n",
    "    } for i in search_results]\n",
    "    return documents\n",
    "\n",
    "# Create React agent\n",
    "memory = MemorySaver()\n",
    "tools = [search_index]\n",
    "prompt = \"\"\"You are a helpful assistant specializing in React and web development. \n",
    "You should use the search_index tool to search for relevant information in the knowledge base \n",
    "to answer user questions accurately. Always provide citations from the search results when available.\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory, version=\"v2\", prompt=prompt)\n",
    "\n",
    "print(\"‚úÖ React agent created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agent(question: str, thread_id: str = \"eval_thread\") -> tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Query the React agent and extract answer and citations.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask the agent\n",
    "        thread_id: Thread ID for conversation memory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (answer, citations)\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Get response from agent\n",
    "    response_chunks = []\n",
    "    for chunk in agent_executor.stream({\"messages\": question}, stream_mode=\"messages\", config=config):\n",
    "        if hasattr(chunk[0], 'content') and chunk[0].content:\n",
    "            response_chunks.append(chunk[0].content)\n",
    "    \n",
    "    answer = ' '.join(response_chunks).strip()\n",
    "    \n",
    "    # Extract citations (simplified - looking for common citation patterns)\n",
    "    citations = []\n",
    "    if 'React' in answer or 'handbook' in answer.lower():\n",
    "        citations.append('React-handbook.pdf')\n",
    "    \n",
    "    return answer, citations\n",
    "\n",
    "def extract_context_from_search(question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract context by directly calling the search tool.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to search for\n",
    "        \n",
    "    Returns:\n",
    "        List of context strings\n",
    "    \"\"\"\n",
    "    search_results = search_index(question)\n",
    "    contexts = [doc['content'] for doc in search_results if doc['content']]\n",
    "    return contexts[:5]  # Limit to top 5 results\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Custom citation accuracy metric using G-Eval\n",
    "citation_accuracy_metric = GEval(\n",
    "    name=\"Citation Accuracy\",\n",
    "    criteria=\"Determine if the actual answer includes proper citations that match the expected citations. Check if the sources referenced are accurate and relevant.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the actual answer contains citations\",\n",
    "        \"Compare the citations in actual answer with expected citations\", \n",
    "        \"Evaluate if the citations are relevant to the content\",\n",
    "        \"Assign a score from 0 to 1 based on citation accuracy\"\n",
    "    ],\n",
    "    evaluation_params=[\n",
    "        \"actual_answer\",\n",
    "        \"expected_citation\"\n",
    "    ],\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DeepEval metrics initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation questions from CSV\n",
    "df = pd.read_csv('evaluation_questions.csv')\n",
    "print(f\"Loaded {len(df)} evaluation questions\")\n",
    "print(\"\\nSample questions:\")\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"Q{i+1}: {row['question']}\")\n",
    "    print(f\"Expected: {row['expected_answer'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all questions\n",
    "results = []\n",
    "test_cases = []\n",
    "\n",
    "print(\"Starting evaluation...\\n\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    question = row['question']\n",
    "    expected_answer = row['expected_answer']\n",
    "    expected_citation = row['expected_citation']\n",
    "    \n",
    "    print(f\"Evaluating question {idx + 1}/{len(df)}: {question[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get agent response\n",
    "        actual_answer, actual_citations = query_agent(question, thread_id=f\"eval_{idx}\")\n",
    "        \n",
    "        # Get context for evaluation\n",
    "        retrieval_context = extract_context_from_search(question)\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=question,\n",
    "            actual_output=actual_answer,\n",
    "            expected_output=expected_answer,\n",
    "            retrieval_context=retrieval_context\n",
    "        )\n",
    "        \n",
    "        test_cases.append(test_case)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'expected_answer': expected_answer,\n",
    "            'expected_citation': expected_citation,\n",
    "            'actual_answer': actual_answer,\n",
    "            'actual_citation': '; '.join(actual_citations),\n",
    "            'no_of_citation_correct': len([c for c in actual_citations if expected_citation in c])\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"‚úÖ Question {idx + 1} completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing question {idx + 1}: {str(e)}\")\n",
    "        # Add empty result to maintain alignment\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'expected_answer': expected_answer,\n",
    "            'expected_citation': expected_citation,\n",
    "            'actual_answer': f\"Error: {str(e)}\",\n",
    "            'actual_citation': '',\n",
    "            'no_of_citation_correct': 0\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Completed querying agent for {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running DeepEval metrics evaluation...\\n\")\n",
    "\n",
    "# Evaluate each test case with metrics\n",
    "for i, (test_case, result) in enumerate(zip(test_cases, results)):\n",
    "    if \"Error:\" in result['actual_answer']:\n",
    "        # Skip evaluation for error cases\n",
    "        result.update({\n",
    "            'answer_relevancy_score': 0.0,\n",
    "            'faithfulness_score': 0.0,\n",
    "            'contextual_relevancy_score': 0.0,\n",
    "            'citation_accuracy_score': 0.0,\n",
    "            'correctness': 'Failed',\n",
    "            'answer_score': 0.0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    print(f\"Evaluating metrics for question {i + 1}...\")\n",
    "    \n",
    "    try:\n",
    "        # Evaluate individual metrics\n",
    "        metrics_to_evaluate = [answer_relevancy_metric, faithfulness_metric, contextual_relevancy_metric]\n",
    "        \n",
    "        # Run evaluation\n",
    "        evaluation_results = evaluate(\n",
    "            test_cases=[test_case],\n",
    "            metrics=metrics_to_evaluate,\n",
    "            print_results=False\n",
    "        )\n",
    "        \n",
    "        # Extract scores\n",
    "        eval_result = evaluation_results[0]\n",
    "        \n",
    "        answer_relevancy_score = next((m.score for m in eval_result.metrics_metadata if m.metric == \"Answer Relevancy\"), 0.0)\n",
    "        faithfulness_score = next((m.score for m in eval_result.metrics_metadata if m.metric == \"Faithfulness\"), 0.0)\n",
    "        contextual_relevancy_score = next((m.score for m in eval_result.metrics_metadata if m.metric == \"Contextual Relevancy\"), 0.0)\n",
    "        \n",
    "        # Calculate overall answer score (average of metrics)\n",
    "        answer_score = (answer_relevancy_score + faithfulness_score + contextual_relevancy_score) / 3\n",
    "        \n",
    "        # Determine correctness based on thresholds\n",
    "        correctness = \"Pass\" if answer_score >= 0.7 else \"Fail\"\n",
    "        \n",
    "        # Simple citation accuracy check\n",
    "        citation_accuracy_score = 1.0 if result['expected_citation'] in result['actual_citation'] else 0.0\n",
    "        \n",
    "        # Update result with scores\n",
    "        result.update({\n",
    "            'answer_relevancy_score': round(answer_relevancy_score, 3),\n",
    "            'faithfulness_score': round(faithfulness_score, 3),\n",
    "            'contextual_relevancy_score': round(contextual_relevancy_score, 3),\n",
    "            'citation_accuracy_score': round(citation_accuracy_score, 3),\n",
    "            'correctness': correctness,\n",
    "            'answer_score': round(answer_score, 3)\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Question {i + 1} evaluated - Score: {answer_score:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating question {i + 1}: {str(e)}\")\n",
    "        result.update({\n",
    "            'answer_relevancy_score': 0.0,\n",
    "            'faithfulness_score': 0.0,\n",
    "            'contextual_relevancy_score': 0.0,\n",
    "            'citation_accuracy_score': 0.0,\n",
    "            'correctness': 'Failed',\n",
    "            'answer_score': 0.0\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Completed evaluation with DeepEval metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns to match the requested format\n",
    "column_order = [\n",
    "    'question', 'expected_answer', 'expected_citation', \n",
    "    'actual_answer', 'actual_citation', 'correctness', \n",
    "    'answer_score', 'no_of_citation_correct',\n",
    "    'answer_relevancy_score', 'faithfulness_score', \n",
    "    'contextual_relevancy_score', 'citation_accuracy_score'\n",
    "]\n",
    "\n",
    "# Add any missing columns\n",
    "for col in column_order:\n",
    "    if col not in results_df.columns:\n",
    "        results_df[col] = ''\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'react_agent_evaluation_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Also save to Excel for better formatting\n",
    "excel_file = 'react_agent_evaluation_results.xlsx'\n",
    "results_df.to_excel(excel_file, index=False, sheet_name='Evaluation Results')\n",
    "\n",
    "print(f\"‚úÖ Results saved to {output_file} and {excel_file}\")\n",
    "print(f\"\\nEvaluation Summary:\")\n",
    "print(f\"Total questions: {len(results_df)}\")\n",
    "print(f\"Passed: {len(results_df[results_df['correctness'] == 'Pass'])}\")\n",
    "print(f\"Failed: {len(results_df[results_df['correctness'] == 'Fail'])}\")\n",
    "print(f\"Errors: {len(results_df[results_df['correctness'] == 'Failed'])}\")\n",
    "print(f\"Average Answer Score: {results_df['answer_score'].mean():.3f}\")\n",
    "print(f\"Average Citation Accuracy: {results_df['citation_accuracy_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "print(\"\\nüìä Detailed Evaluation Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\nQuestion {idx + 1}: {row['question']}\")\n",
    "    print(f\"Expected Answer: {row['expected_answer'][:100]}...\")\n",
    "    print(f\"Actual Answer: {row['actual_answer'][:100]}...\")\n",
    "    print(f\"Expected Citation: {row['expected_citation']}\")\n",
    "    print(f\"Actual Citation: {row['actual_citation']}\")\n",
    "    print(f\"Correctness: {row['correctness']}\")\n",
    "    print(f\"Answer Score: {row['answer_score']}\")\n",
    "    print(f\"Citation Accuracy: {row['citation_accuracy_score']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Show metric breakdown\n",
    "print(\"\\nüìà Metric Breakdown:\")\n",
    "print(f\"Answer Relevancy: {results_df['answer_relevancy_score'].mean():.3f} ¬± {results_df['answer_relevancy_score'].std():.3f}\")\n",
    "print(f\"Faithfulness: {results_df['faithfulness_score'].mean():.3f} ¬± {results_df['faithfulness_score'].std():.3f}\")\n",
    "print(f\"Contextual Relevancy: {results_df['contextual_relevancy_score'].mean():.3f} ¬± {results_df['contextual_relevancy_score'].std():.3f}\")\n",
    "print(f\"Citation Accuracy: {results_df['citation_accuracy_score'].mean():.3f} ¬± {results_df['citation_accuracy_score'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create visualizations if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('React Agent Evaluation Results', fontsize=16)\n",
    "    \n",
    "    # Metric scores distribution\n",
    "    metrics = ['answer_relevancy_score', 'faithfulness_score', 'contextual_relevancy_score', 'citation_accuracy_score']\n",
    "    metric_names = ['Answer Relevancy', 'Faithfulness', 'Contextual Relevancy', 'Citation Accuracy']\n",
    "    \n",
    "    for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "        ax = axes[i//2, i%2]\n",
    "        ax.hist(results_df[metric], bins=10, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'{name} Distribution')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.axvline(results_df[metric].mean(), color='red', linestyle='--', label=f'Mean: {results_df[metric].mean():.3f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_results_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Correctness pie chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correctness_counts = results_df['correctness'].value_counts()\n",
    "    plt.pie(correctness_counts.values, labels=correctness_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Evaluation Results Distribution')\n",
    "    plt.savefig('correctness_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations saved as PNG files\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"üìä Matplotlib not available. Install it for visualizations: pip install matplotlib seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a comprehensive evaluation approach for LLM agents using DeepEval. The evaluation covers:\n",
    "\n",
    "1. **Answer Quality**: Using DeepEval's built-in metrics (Answer Relevancy, Faithfulness, Contextual Relevancy)\n",
    "2. **Citation Accuracy**: Custom evaluation of whether the agent provides proper citations\n",
    "3. **Systematic Testing**: Batch evaluation of multiple questions from CSV/Excel files\n",
    "4. **Results Export**: Saving detailed results back to Excel for further analysis\n",
    "\n",
    "### Key Features:\n",
    "- **Automated Evaluation**: No manual scoring required\n",
    "- **Multiple Metrics**: Comprehensive assessment across different dimensions\n",
    "- **Reproducible**: Can be run multiple times with consistent results\n",
    "- **Extensible**: Easy to add new questions or modify evaluation criteria\n",
    "\n",
    "### Next Steps:\n",
    "1. Analyze the results to identify areas for improvement\n",
    "2. Tune the agent's prompt or search parameters based on evaluation findings\n",
    "3. Add more diverse questions to the evaluation set\n",
    "4. Implement automated evaluation as part of CI/CD pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
